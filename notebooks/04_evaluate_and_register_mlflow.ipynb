{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bca015f1",
   "metadata": {},
   "source": [
    "# Cuarto notebook\n",
    "Aquí se verá: cargar configuración central, cargar el mejor checkpoint entrenado, evaluar en validación con métricas prácticas (Precision/Recall/F1 y AP aproximado @ IoU=0.5) sobre un subconjunto (CPU-safe), registrar métricas en MLflow y registrar el modelo en el Model Registry (usando SQLite).\n",
    "## Objetivos\n",
    "1. Cargar `project_config.json` y `labelmap.json`.\n",
    "2. Localizar el mejor checkpoint guardado (best_*.pt) en `models/local_checkpoints/`.\n",
    "3. Cargar el modelo y ejecutar inferencia sobre un subconjunto de validación (CPU-safe).\n",
    "4. Calcular métricas:\n",
    "   - Precision / Recall / F1 (por clase y global) con IoU >= 0.5\n",
    "   - AP aproximado @0.5 por clase (por ranking de score)\n",
    "5. Loggear resultados en MLflow (SQLite):\n",
    "   - parámetros de evaluación\n",
    "   - métricas globales y por clase\n",
    "   - artefactos (checkpoint evaluado + reporte)\n",
    "6. Registrar el modelo en MLflow Model Registry con nombre estable y versión nueva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69aa4953",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[WinError 1114] Error en una rutina de inicialización de biblioteca de vínculos dinámicos (DLL). Error loading \"c:\\Users\\Johnny\\AppData\\Local\\Python\\pythoncore-3.12-64\\Lib\\site-packages\\torch\\lib\\c10.dll\" or one of its dependencies.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmlflow\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmlflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtracking\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MlflowClient\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdetection\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfaster_rcnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FastRCNNPredictor\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Johnny\\AppData\\Local\\Python\\pythoncore-3.12-64\\Lib\\site-packages\\torch\\__init__.py:281\u001b[39m\n\u001b[32m    277\u001b[39m                     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[32m    279\u001b[39m         kernel32.SetErrorMode(prev_error_mode)\n\u001b[32m--> \u001b[39m\u001b[32m281\u001b[39m     \u001b[43m_load_dll_libraries\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    282\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m _load_dll_libraries\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_cuda_dep_paths\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m, lib_folder: \u001b[38;5;28mstr\u001b[39m, lib_name: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    286\u001b[39m     \u001b[38;5;66;03m# Libraries can either be in\u001b[39;00m\n\u001b[32m    287\u001b[39m     \u001b[38;5;66;03m# path/nvidia/lib_folder/lib or\u001b[39;00m\n\u001b[32m    288\u001b[39m     \u001b[38;5;66;03m# path/nvidia/cuXX/lib (since CUDA 13.0) or\u001b[39;00m\n\u001b[32m    289\u001b[39m     \u001b[38;5;66;03m# path/lib_folder/lib\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Johnny\\AppData\\Local\\Python\\pythoncore-3.12-64\\Lib\\site-packages\\torch\\__init__.py:264\u001b[39m, in \u001b[36m_load_dll_libraries\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    260\u001b[39m     err = ctypes.WinError(last_error)\n\u001b[32m    261\u001b[39m     err.strerror += (\n\u001b[32m    262\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m Error loading \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdll\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m or one of its dependencies.\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    263\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m264\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[32m    265\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    266\u001b[39m     is_loaded = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mOSError\u001b[39m: [WinError 1114] Error en una rutina de inicialización de biblioteca de vínculos dinámicos (DLL). Error loading \"c:\\Users\\Johnny\\AppData\\Local\\Python\\pythoncore-3.12-64\\Lib\\site-packages\\torch\\lib\\c10.dll\" or one of its dependencies."
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "- Importa librerías necesarias para evaluación, carga del modelo y MLflow registry.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.transforms import functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291b6ada",
   "metadata": {},
   "source": [
    "1. Carga de archivos .json y validacion de estructura del proyecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8879e8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: C:\\Users\\Johnny\\Desktop\\IA-final\n",
      "VAL_JSON: C:\\Users\\Johnny\\Desktop\\IA-final\\data\\processed\\coco_person_car_airplane_val.json\n",
      "TARGET_CLASSES: ['person', 'car', 'airplane']\n",
      "target_cat_ids: [1, 3, 5]\n",
      "MODELS_DIR: C:\\Users\\Johnny\\Desktop\\IA-final\\models\\local_checkpoints\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "En esta secccion se  Carga project_config.json y labelmap.json creados en el Notebook 01.\n",
    "Aemas define rutas absolutas sin depender de la carpeta actual del notebook.\n",
    "\"\"\"\n",
    "\n",
    "def find_project_root(start: Path, max_up: int = 8) -> Path:\n",
    "    cur = start.resolve()\n",
    "    for _ in range(max_up):\n",
    "        if (cur / \"data\" / \"processed\" / \"project_config.json\").exists():\n",
    "            return cur\n",
    "        cur = cur.parent\n",
    "    raise FileNotFoundError(\"No se encontró data/processed/project_config.json. Ejecuta Notebook 01.\")\n",
    "\n",
    "PROJECT_ROOT = find_project_root(Path.cwd())\n",
    "PROCESSED_DIR = (PROJECT_ROOT / \"data\" / \"processed\").resolve()\n",
    "\n",
    "PROJECT_CONFIG_PATH = (PROCESSED_DIR / \"project_config.json\").resolve()\n",
    "LABELMAP_PATH = (PROCESSED_DIR / \"labelmap.json\").resolve()\n",
    "\n",
    "with open(PROJECT_CONFIG_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    project_config = json.load(f)\n",
    "\n",
    "with open(LABELMAP_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    labelmap = json.load(f)\n",
    "\n",
    "TRAIN_IMG_DIR = Path(project_config[\"train_dir\"])\n",
    "VAL_IMG_DIR = Path(project_config[\"val_dir\"])\n",
    "\n",
    "TARGET_CLASSES = project_config[\"target_classes\"]\n",
    "target_cat_ids = [int(x) for x in labelmap[\"target_category_ids\"]]\n",
    "\n",
    "VAL_JSON = (PROCESSED_DIR / \"coco_person_car_airplane_val.json\").resolve()\n",
    "\n",
    "MODELS_DIR = (PROJECT_ROOT / \"models\" / \"local_checkpoints\").resolve()\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"VAL_JSON:\", VAL_JSON)\n",
    "print(\"TARGET_CLASSES:\", TARGET_CLASSES)\n",
    "print(\"target_cat_ids:\", target_cat_ids)\n",
    "print(\"MODELS_DIR:\", MODELS_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b14ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST_CKPT_PATH: C:\\Users\\Johnny\\Desktop\\IA-final\\models\\local_checkpoints\\best_frcnn_cpu_base_train_20260201_083448.pt\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Esta celda:\n",
    "- Busca el archivo best_*.pt más reciente en MODELS_DIR.\n",
    "- Falla con error claro si no existe ninguno.\n",
    "\"\"\"\n",
    "\n",
    "def find_latest_best_checkpoint(models_dir: Path) -> Path:\n",
    "    cands = sorted(models_dir.glob(\"best_*.pt\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    if not cands:\n",
    "        raise FileNotFoundError(\n",
    "            \"No se encontró ningún checkpoint best_*.pt en models/local_checkpoints. \"\n",
    "            \"Asegúrate de entrenar y guardar checkpoints en el Notebook 03.\"\n",
    "        )\n",
    "    return cands[0]\n",
    "\n",
    "BEST_CKPT_PATH = find_latest_best_checkpoint(MODELS_DIR)\n",
    "print(\"BEST_CKPT_PATH:\", BEST_CKPT_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0be4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint classes: ['person', 'car', 'airplane']\n",
      "NUM_CLASSES: 4\n",
      "coco_to_internal: {1: 1, 3: 2, 5: 3}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Esta celda:\n",
    "- Carga el checkpoint.\n",
    "- Reconstruye el modelo Faster R-CNN con la cantidad correcta de clases.\n",
    "- Carga state_dict y deja el modelo en eval() para inferencia.\n",
    "\"\"\"\n",
    "\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "def build_model(num_classes: int):\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=None)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "ckpt = torch.load(BEST_CKPT_PATH, map_location=\"cpu\")\n",
    "\n",
    "ckpt_target_classes = ckpt[\"target_classes\"]\n",
    "NUM_CLASSES = len(ckpt_target_classes) + 1\n",
    "\n",
    "coco_to_internal = {int(k): int(v) for k, v in ckpt[\"coco_to_internal\"].items()} if isinstance(next(iter(ckpt[\"coco_to_internal\"].keys())), str) else ckpt[\"coco_to_internal\"]\n",
    "internal_to_name = {int(k): v for k, v in ckpt[\"internal_to_name\"].items()} if isinstance(next(iter(ckpt[\"internal_to_name\"].keys())), str) else ckpt[\"internal_to_name\"]\n",
    "\n",
    "model = build_model(NUM_CLASSES)\n",
    "model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "model.to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "print(\"Checkpoint classes:\", ckpt_target_classes)\n",
    "print(\"NUM_CLASSES:\", NUM_CLASSES)\n",
    "print(\"coco_to_internal:\", coco_to_internal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557e56e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val images: 500\n",
      "Val annotations: 2218\n",
      "Ejemplo image_id: 397133 file: 000000397133.jpg\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Esta celda:\n",
    "- Carga el JSON reducido de validación (Notebook 02).\n",
    "- Construye índices: image_id -> file_name y image_id -> GT boxes por clase interna.\n",
    "- Convierte bbox COCO (x,y,w,h) a (x1,y1,x2,y2).\n",
    "- Remapea category_id COCO a label interno 1..K.\n",
    "\"\"\"\n",
    "\n",
    "if not VAL_JSON.exists():\n",
    "    raise FileNotFoundError(f\"No existe VAL_JSON: {VAL_JSON}. Ejecuta Notebook 02.\")\n",
    "\n",
    "with open(VAL_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    val_coco = json.load(f)\n",
    "\n",
    "val_images = val_coco[\"images\"]\n",
    "val_anns = val_coco[\"annotations\"]\n",
    "\n",
    "img_id_to_file = {img[\"id\"]: img[\"file_name\"] for img in val_images}\n",
    "\n",
    "gt_by_image: Dict[int, Dict[int, List[List[float]]]] = {}  # image_id -> internal_label -> [xyxy]\n",
    "for ann in val_anns:\n",
    "    img_id = ann[\"image_id\"]\n",
    "    cid = int(ann[\"category_id\"])\n",
    "    if cid not in coco_to_internal:\n",
    "        continue\n",
    "    internal_label = int(coco_to_internal[cid])\n",
    "    x, y, w, h = ann[\"bbox\"]\n",
    "    box = [x, y, x + w, y + h]\n",
    "    gt_by_image.setdefault(img_id, {}).setdefault(internal_label, []).append(box)\n",
    "\n",
    "print(\"Val images:\", len(val_images))\n",
    "print(\"Val annotations:\", len(val_anns))\n",
    "print(\"Ejemplo image_id:\", val_images[0][\"id\"], \"file:\", val_images[0][\"file_name\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde1e4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Esta celda:\n",
    "- Implementa IoU para boxes xyxy.\n",
    "- Implementa matching greedy por clase:\n",
    "  - un GT solo puede emparejarse con una predicción\n",
    "  - IoU >= threshold => TP, si no => FP\n",
    "  - GT no emparejado => FN\n",
    "\"\"\"\n",
    "\n",
    "def iou_xyxy(a: List[float], b: List[float]) -> float:\n",
    "    ax1, ay1, ax2, ay2 = a\n",
    "    bx1, by1, bx2, by2 = b\n",
    "\n",
    "    inter_x1 = max(ax1, bx1)\n",
    "    inter_y1 = max(ay1, by1)\n",
    "    inter_x2 = min(ax2, bx2)\n",
    "    inter_y2 = min(ay2, by2)\n",
    "\n",
    "    inter_w = max(0.0, inter_x2 - inter_x1)\n",
    "    inter_h = max(0.0, inter_y2 - inter_y1)\n",
    "    inter_area = inter_w * inter_h\n",
    "\n",
    "    area_a = max(0.0, ax2 - ax1) * max(0.0, ay2 - ay1)\n",
    "    area_b = max(0.0, bx2 - bx1) * max(0.0, by2 - by1)\n",
    "\n",
    "    union = area_a + area_b - inter_area\n",
    "    return 0.0 if union <= 0 else inter_area / union\n",
    "\n",
    "def match_predictions_to_gt(\n",
    "    preds: List[Tuple[List[float], float]],  # (box, score) for a class\n",
    "    gts: List[List[float]],\n",
    "    iou_thr: float\n",
    ") -> Tuple[int, int, int, List[Tuple[float, int]]]:\n",
    "    \"\"\"\n",
    "    Retorna:\n",
    "    - TP, FP, FN\n",
    "    - ranked list (score, is_tp) para AP aproximado\n",
    "    \"\"\"\n",
    "    preds_sorted = sorted(preds, key=lambda x: x[1], reverse=True)\n",
    "    gt_used = [False] * len(gts)\n",
    "\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    ranked = []\n",
    "\n",
    "    for p_box, p_score in preds_sorted:\n",
    "        best_iou = 0.0\n",
    "        best_j = -1\n",
    "        for j, gt_box in enumerate(gts):\n",
    "            if gt_used[j]:\n",
    "                continue\n",
    "            cur_iou = iou_xyxy(p_box, gt_box)\n",
    "            if cur_iou > best_iou:\n",
    "                best_iou = cur_iou\n",
    "                best_j = j\n",
    "\n",
    "        if best_iou >= iou_thr and best_j >= 0:\n",
    "            gt_used[best_j] = True\n",
    "            tp += 1\n",
    "            ranked.append((p_score, 1))\n",
    "        else:\n",
    "            fp += 1\n",
    "            ranked.append((p_score, 0))\n",
    "\n",
    "    fn = sum(1 for u in gt_used if not u)\n",
    "    return tp, fp, fn, ranked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d11a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "eval_infer: 100%|██████████| 300/300 [7:13:53<00:00, 86.78s/it]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global micro metrics:\n",
      "TP: 900 FP: 877 FN: 346\n",
      "Precision: 0.5064715813168261\n",
      "Recall   : 0.7223113964686998\n",
      "F1       : 0.5954349983460139\n",
      "\n",
      "Por clase:\n",
      "person | P: 0.5 R: 0.7399 F1: 0.5967 GT: 1111\n",
      "car | P: 0.5726 R: 0.5826 F1: 0.5776 GT: 115\n",
      "airplane | P: 0.6875 R: 0.55 F1: 0.6111 GT: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Esta celda:\n",
    "- Ejecuta inferencia sobre un subconjunto de imágenes de validación para no reventar CPU.\n",
    "- Filtra por score_threshold.\n",
    "- Calcula TP/FP/FN por clase con IoU >= 0.5.\n",
    "- Calcula Precision/Recall/F1 por clase y global.\n",
    "- Calcula AP aproximado por clase (métrica práctica por ranking de score).\n",
    "\"\"\"\n",
    "\n",
    "EVAL_MAX_IMAGES = 300   \n",
    "SCORE_THRESHOLD = 0.5\n",
    "IOU_THRESHOLD = 0.5\n",
    "\n",
    "val_image_ids = [img[\"id\"] for img in val_images]\n",
    "eval_ids = val_image_ids[:min(EVAL_MAX_IMAGES, len(val_image_ids))]\n",
    "\n",
    "# stats por clase interna: 1..K\n",
    "K = len(TARGET_CLASSES)\n",
    "tp_c = {c: 0 for c in range(1, K + 1)}\n",
    "fp_c = {c: 0 for c in range(1, K + 1)}\n",
    "fn_c = {c: 0 for c in range(1, K + 1)}\n",
    "\n",
    "# para AP aproximado: lista de (score, is_tp) por clase\n",
    "ranked_c: Dict[int, List[Tuple[float, int]]] = {c: [] for c in range(1, K + 1)}\n",
    "gt_count_c = {c: 0 for c in range(1, K + 1)}  # total GT por clase en subset\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_image(model, img_path: Path):\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    img_t = F.to_tensor(img).to(DEVICE)\n",
    "    out = model([img_t])[0]\n",
    "    return out\n",
    "\n",
    "for img_id in tqdm(eval_ids, desc=\"eval_infer\"):\n",
    "    file_name = img_id_to_file[img_id]\n",
    "    img_path = VAL_IMG_DIR / file_name\n",
    "\n",
    "    out = predict_image(model, img_path)\n",
    "\n",
    "    boxes = out[\"boxes\"].cpu().tolist()\n",
    "    scores = out[\"scores\"].cpu().tolist()\n",
    "    labels = out[\"labels\"].cpu().tolist()  # interno 1..K (según checkpoint)\n",
    "\n",
    "    # agrupar preds por clase\n",
    "    preds_by_class: Dict[int, List[Tuple[List[float], float]]] = {c: [] for c in range(1, K + 1)}\n",
    "    for b, s, l in zip(boxes, scores, labels):\n",
    "        l = int(l)\n",
    "        if l < 1 or l > K:\n",
    "            continue\n",
    "        if s >= SCORE_THRESHOLD:\n",
    "            preds_by_class[l].append((b, float(s)))\n",
    "\n",
    "    # GT por clase\n",
    "    gt_for_img = gt_by_image.get(img_id, {})\n",
    "    for c in range(1, K + 1):\n",
    "        gts = gt_for_img.get(c, [])\n",
    "        gt_count_c[c] += len(gts)\n",
    "        preds = preds_by_class.get(c, [])\n",
    "\n",
    "        tpi, fpi, fni, ranked = match_predictions_to_gt(preds, gts, IOU_THRESHOLD)\n",
    "        tp_c[c] += tpi\n",
    "        fp_c[c] += fpi\n",
    "        fn_c[c] += fni\n",
    "        ranked_c[c].extend(ranked)\n",
    "\n",
    "def safe_div(a: float, b: float) -> float:\n",
    "    return 0.0 if b == 0 else a / b\n",
    "\n",
    "metrics_by_class = {}\n",
    "for c in range(1, K + 1):\n",
    "    tp = tp_c[c]\n",
    "    fp = fp_c[c]\n",
    "    fn = fn_c[c]\n",
    "    prec = safe_div(tp, tp + fp)\n",
    "    rec = safe_div(tp, tp + fn)\n",
    "    f1 = safe_div(2 * prec * rec, prec + rec) if (prec + rec) > 0 else 0.0\n",
    "\n",
    "    metrics_by_class[c] = {\n",
    "        \"name\": internal_to_name.get(c, f\"class_{c}\"),\n",
    "        \"tp\": tp, \"fp\": fp, \"fn\": fn,\n",
    "        \"precision\": prec,\n",
    "        \"recall\": rec,\n",
    "        \"f1\": f1,\n",
    "        \"gt_count\": gt_count_c[c],\n",
    "        \"pred_count\": tp + fp,\n",
    "    }\n",
    "\n",
    "# global micro\n",
    "TP = sum(tp_c.values())\n",
    "FP = sum(fp_c.values())\n",
    "FN = sum(fn_c.values())\n",
    "P_micro = safe_div(TP, TP + FP)\n",
    "R_micro = safe_div(TP, TP + FN)\n",
    "F1_micro = safe_div(2 * P_micro * R_micro, P_micro + R_micro) if (P_micro + R_micro) > 0 else 0.0\n",
    "\n",
    "print(\"Global micro metrics:\")\n",
    "print(\"TP:\", TP, \"FP:\", FP, \"FN:\", FN)\n",
    "print(\"Precision:\", P_micro)\n",
    "print(\"Recall   :\", R_micro)\n",
    "print(\"F1       :\", F1_micro)\n",
    "\n",
    "print(\"\\nPor clase:\")\n",
    "for c in range(1, K + 1):\n",
    "    m = metrics_by_class[c]\n",
    "    print(m[\"name\"], \"| P:\", round(m[\"precision\"], 4), \"R:\", round(m[\"recall\"], 4), \"F1:\", round(m[\"f1\"], 4), \"GT:\", m[\"gt_count\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4da5e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AP@0.5 aproximado por clase:\n",
      "person AP: 0.6368\n",
      "car AP: 0.4877\n",
      "airplane AP: 0.55\n",
      "\n",
      "mAP@0.5 aproximado: 0.5581\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Esta celda:\n",
    "- Calcula AP aproximado por clase usando la lista rankeada (score, is_tp).\n",
    "- AP aquí es aproximado (no COCO mAP), pero sirve para comparar versiones y registrar en MLflow.\n",
    "\"\"\"\n",
    "\n",
    "def average_precision_from_ranked(ranked: List[Tuple[float, int]], total_gt: int) -> float:\n",
    "    if total_gt == 0:\n",
    "        return 0.0\n",
    "    if not ranked:\n",
    "        return 0.0\n",
    "\n",
    "    ranked_sorted = sorted(ranked, key=lambda x: x[0], reverse=True)\n",
    "    tp_running = 0\n",
    "    fp_running = 0\n",
    "\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "\n",
    "    for _, is_tp in ranked_sorted:\n",
    "        if is_tp == 1:\n",
    "            tp_running += 1\n",
    "        else:\n",
    "            fp_running += 1\n",
    "        prec = safe_div(tp_running, tp_running + fp_running)\n",
    "        rec = safe_div(tp_running, total_gt)\n",
    "        precisions.append(prec)\n",
    "        recalls.append(rec)\n",
    "\n",
    "    # AP por integración tipo \"step\": suma de precision en puntos donde recall incrementa\n",
    "    ap = 0.0\n",
    "    prev_rec = 0.0\n",
    "    for p, r in zip(precisions, recalls):\n",
    "        if r > prev_rec:\n",
    "            ap += p * (r - prev_rec)\n",
    "            prev_rec = r\n",
    "    return ap\n",
    "\n",
    "ap_by_class = {}\n",
    "for c in range(1, K + 1):\n",
    "    ap = average_precision_from_ranked(ranked_c[c], gt_count_c[c])\n",
    "    ap_by_class[c] = ap\n",
    "\n",
    "mAP = sum(ap_by_class.values()) / max(1, K)\n",
    "\n",
    "print(\"AP@0.5 aproximado por clase:\")\n",
    "for c in range(1, K + 1):\n",
    "    print(internal_to_name.get(c, f\"class_{c}\"), \"AP:\", round(ap_by_class[c], 4))\n",
    "\n",
    "print(\"\\nmAP@0.5 aproximado:\", round(mAP, 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6945715",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/02 08:53:49 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.schemas\n",
      "2026/02/02 08:53:49 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.tables\n",
      "2026/02/02 08:53:49 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.types\n",
      "2026/02/02 08:53:49 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.constraints\n",
      "2026/02/02 08:53:49 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.defaults\n",
      "2026/02/02 08:53:49 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.comments\n",
      "2026/02/02 08:53:49 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n",
      "2026/02/02 08:53:49 INFO mlflow.store.db.utils: Updating database tables\n",
      "2026/02/02 08:53:49 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n",
      "2026/02/02 08:53:49 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n",
      "2026/02/02 08:53:50 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n",
      "2026/02/02 08:53:50 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n",
      "2026/02/02 08:53:50 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n",
      "2026/02/02 08:53:50 INFO mlflow.store.db.utils: Updating database tables\n",
      "2026/02/02 08:53:50 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n",
      "2026/02/02 08:53:50 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluación registrada en MLflow.\n",
      "Registered model: frcnn_coco_cpu_person_car_airplane\n",
      "Checkpoint registrado: best_frcnn_cpu_base_train_20260201_083448.pt\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Esta celda:\n",
    "- Configura MLflow en SQLite.\n",
    "- Crea un run de evaluación.\n",
    "- Loggea parámetros y métricas globales y por clase.\n",
    "- Loggea artefactos: checkpoint, reporte JSON.\n",
    "- Registra el modelo en Model Registry:\n",
    "  - nombre estable: frcnn_coco_cpu_person_car_airplane\n",
    "  - crea una nueva versión apuntando al artefacto del run.\n",
    "\"\"\"\n",
    "\n",
    "MLFLOW_DB = (PROJECT_ROOT / \"mlflow.db\").resolve()\n",
    "mlflow.set_tracking_uri(f\"sqlite:///{MLFLOW_DB.as_posix()}\")\n",
    "\n",
    "EXPERIMENT_NAME = \"object_detection_coco_cpu\"\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "client = MlflowClient()\n",
    "\n",
    "registered_model_name = \"frcnn_coco_cpu_person_car_airplane\"\n",
    "\n",
    "# crear registered model si no existe\n",
    "existing = [m.name for m in client.search_registered_models()]\n",
    "if registered_model_name not in existing:\n",
    "    client.create_registered_model(registered_model_name)\n",
    "\n",
    "eval_run_name = f\"eval_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "report = {\n",
    "    \"checkpoint_path\": str(BEST_CKPT_PATH),\n",
    "    \"eval_max_images\": EVAL_MAX_IMAGES,\n",
    "    \"score_threshold\": SCORE_THRESHOLD,\n",
    "    \"iou_threshold\": IOU_THRESHOLD,\n",
    "    \"global\": {\n",
    "        \"tp\": TP, \"fp\": FP, \"fn\": FN,\n",
    "        \"precision_micro\": P_micro,\n",
    "        \"recall_micro\": R_micro,\n",
    "        \"f1_micro\": F1_micro,\n",
    "        \"mAP50_approx\": mAP,\n",
    "    },\n",
    "    \"by_class\": metrics_by_class,\n",
    "    \"ap50_approx_by_class\": {internal_to_name.get(c, str(c)): ap_by_class[c] for c in ap_by_class},\n",
    "}\n",
    "\n",
    "report_path = (MODELS_DIR / f\"eval_report_{eval_run_name}.json\").resolve()\n",
    "with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(report, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "with mlflow.start_run(run_name=eval_run_name) as run:\n",
    "    run_id = run.info.run_id\n",
    "\n",
    "    # params\n",
    "    mlflow.log_param(\"eval_max_images\", EVAL_MAX_IMAGES)\n",
    "    mlflow.log_param(\"score_threshold\", SCORE_THRESHOLD)\n",
    "    mlflow.log_param(\"iou_threshold\", IOU_THRESHOLD)\n",
    "    mlflow.set_tag(\"stage\", \"eval\")\n",
    "    mlflow.set_tag(\"checkpoint_used\", BEST_CKPT_PATH.name)\n",
    "    mlflow.set_tag(\"classes\", \",\".join(TARGET_CLASSES))\n",
    "    mlflow.set_tag(\"registered_model_name\", registered_model_name)\n",
    "\n",
    "    # global metrics\n",
    "    mlflow.log_metric(\"precision_micro\", P_micro)\n",
    "    mlflow.log_metric(\"recall_micro\", R_micro)\n",
    "    mlflow.log_metric(\"f1_micro\", F1_micro)\n",
    "    mlflow.log_metric(\"mAP50_approx\", mAP)\n",
    "\n",
    "    # per class metrics\n",
    "    for c in range(1, K + 1):\n",
    "        cname = internal_to_name.get(c, f\"class_{c}\")\n",
    "        m = metrics_by_class[c]\n",
    "        mlflow.log_metric(f\"{cname}_precision\", m[\"precision\"])\n",
    "        mlflow.log_metric(f\"{cname}_recall\", m[\"recall\"])\n",
    "        mlflow.log_metric(f\"{cname}_f1\", m[\"f1\"])\n",
    "        mlflow.log_metric(f\"{cname}_ap50_approx\", ap_by_class[c])\n",
    "\n",
    "    # artifacts\n",
    "    mlflow.log_artifact(str(report_path), artifact_path=\"reports\")\n",
    "    mlflow.log_artifact(str(BEST_CKPT_PATH), artifact_path=\"model_ckpt\")\n",
    "    mlflow.log_artifact(str(PROJECT_CONFIG_PATH), artifact_path=\"artifacts\")\n",
    "    mlflow.log_artifact(str(LABELMAP_PATH), artifact_path=\"artifacts\")\n",
    "\n",
    "    # registrar modelo desde el artefacto en este run\n",
    "    model_uri = f\"runs:/{run_id}/model_ckpt/{BEST_CKPT_PATH.name}\"\n",
    "    mv = client.create_model_version(\n",
    "        name=registered_model_name,\n",
    "        source=model_uri,\n",
    "        run_id=run_id\n",
    "    )\n",
    "\n",
    "    # tags/version notes\n",
    "    client.set_model_version_tag(registered_model_name, mv.version, \"eval_max_images\", str(EVAL_MAX_IMAGES))\n",
    "    client.set_model_version_tag(registered_model_name, mv.version, \"score_threshold\", str(SCORE_THRESHOLD))\n",
    "    client.set_model_version_tag(registered_model_name, mv.version, \"iou_threshold\", str(IOU_THRESHOLD))\n",
    "    client.set_model_version_tag(registered_model_name, mv.version, \"mAP50_approx\", str(mAP))\n",
    "\n",
    "print(\"Evaluación registrada en MLflow.\")\n",
    "print(\"Registered model:\", registered_model_name)\n",
    "print(\"Checkpoint registrado:\", BEST_CKPT_PATH.name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
