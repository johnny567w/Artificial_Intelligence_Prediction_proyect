{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf769173",
   "metadata": {},
   "source": [
    "# Quinto notebook\n",
    "En este ultimo notebook se vera el reentrenamiento incremental solo con imágenes nuevas, rápido en CPU, con checkpoint por época, y registro en MLflow (SQLite). Si no hay datos nuevos, se crea un run en MLflow con estado SKIPPED y no se entrena.\n",
    "## Objetivos\n",
    "1. Leer nuevas imágenes etiquetadas por el usuario desde `data/new_data/` (formato YOLO).\n",
    "2. Entrenar SOLO con esas imágenes nuevas (rápido en CPU).\n",
    "3. Evaluar objetivamente si mejoró usando un subset fijo de validación (val_loss_before vs val_loss_after).\n",
    "4. Registrar en MLflow:\n",
    "   - run incremental\n",
    "   - nueva versión del modelo en Model Registry\n",
    "   - tags de mejora\n",
    "5. Si mejora, promover automáticamente a Production (archivando versiones anteriores).\n",
    "6. Si no hay datos nuevos, no entrenar y registrar run SKIPPED."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d2befeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- Importa librerías necesarias para incremental retrain, evaluación y MLflow Registry.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from mlflow.tracking import MlflowClient\n",
    "import time\n",
    "\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.transforms import functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137fe92e",
   "metadata": {},
   "source": [
    "1. Carga de carpetas previas para el fomrat establecido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa235321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: C:\\Users\\Johnny\\Desktop\\IA-final\n",
      "TARGET_CLASSES: ['person', 'car', 'airplane']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "- Encuentra PROJECT_ROOT a partir de data/processed/project_config.json.\n",
    "- Carga project_config.json y labelmap.json.\n",
    "- Define rutas estándar del proyecto.\n",
    "\"\"\"\n",
    "\n",
    "def find_project_root(start: Path, max_up: int = 8) -> Path:\n",
    "    cur = start.resolve()\n",
    "    for _ in range(max_up):\n",
    "        if (cur / \"data\" / \"processed\" / \"project_config.json\").exists():\n",
    "            return cur\n",
    "        cur = cur.parent\n",
    "    raise FileNotFoundError(\"No se encontró data/processed/project_config.json. Ejecuta Notebook 01.\")\n",
    "\n",
    "PROJECT_ROOT = find_project_root(Path.cwd())\n",
    "PROCESSED_DIR = (PROJECT_ROOT / \"data\" / \"processed\").resolve()\n",
    "\n",
    "PROJECT_CONFIG_PATH = (PROCESSED_DIR / \"project_config.json\").resolve()\n",
    "LABELMAP_PATH = (PROCESSED_DIR / \"labelmap.json\").resolve()\n",
    "\n",
    "with open(PROJECT_CONFIG_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    project_config = json.load(f)\n",
    "\n",
    "with open(LABELMAP_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    labelmap = json.load(f)\n",
    "\n",
    "VAL_IMG_DIR = Path(project_config[\"val_dir\"])\n",
    "TARGET_CLASSES = project_config[\"target_classes\"]\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"TARGET_CLASSES:\", TARGET_CLASSES)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a89398",
   "metadata": {},
   "source": [
    "2. Nuevas carpetas para el manjeo de datos proveniente de la interfaz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "204f4941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEW_IMG_DIR: C:\\Users\\Johnny\\Desktop\\IA-final\\data\\new_data\\images\n",
      "NEW_LBL_DIR: C:\\Users\\Johnny\\Desktop\\IA-final\\data\\new_data\\labels\n",
      "NEW_USED_DIR: C:\\Users\\Johnny\\Desktop\\IA-final\\data\\new_data\\used\n",
      "MANIFEST_PATH: C:\\Users\\Johnny\\Desktop\\IA-final\\data\\new_data\\manifest.json\n",
      "\n",
      "Formato YOLO esperado por label:\n",
      "class_id x_center y_center width height  (todo normalizado 0..1)\n",
      "class_id: 0=person, 1=car, 2=airplane\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "- Define estructura para nuevos datos:\n",
    "  - data/new_data/images/*.jpg|png\n",
    "  - data/new_data/labels/*.txt (YOLO) mismo nombre que la imagen\n",
    "  - data/new_data/used/ (archivo de lo ya usado)\n",
    "  - data/new_data/manifest.json (para no repetir)\n",
    "\"\"\"\n",
    "\n",
    "NEW_DATA_DIR = (PROJECT_ROOT / \"data\" / \"new_data\").resolve()\n",
    "NEW_IMG_DIR = (NEW_DATA_DIR / \"images\").resolve()\n",
    "NEW_LBL_DIR = (NEW_DATA_DIR / \"labels\").resolve()\n",
    "NEW_USED_DIR = (NEW_DATA_DIR / \"used\").resolve()\n",
    "MANIFEST_PATH = (NEW_DATA_DIR / \"manifest.json\").resolve()\n",
    "\n",
    "for d in [NEW_DATA_DIR, NEW_IMG_DIR, NEW_LBL_DIR, NEW_USED_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"NEW_IMG_DIR:\", NEW_IMG_DIR)\n",
    "print(\"NEW_LBL_DIR:\", NEW_LBL_DIR)\n",
    "print(\"NEW_USED_DIR:\", NEW_USED_DIR)\n",
    "print(\"MANIFEST_PATH:\", MANIFEST_PATH)\n",
    "\n",
    "print(\"\\nFormato YOLO esperado por label:\")\n",
    "print(\"class_id x_center y_center width height  (todo normalizado 0..1)\")\n",
    "print(\"class_id: 0=person, 1=car, 2=airplane\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c21db7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: c:\\Users\\Johnny\\Desktop\\IA-final\n",
      "NEW_IMG_DIR : c:\\Users\\Johnny\\Desktop\\IA-final\\data\\new_data\\images\n",
      "NEW_LBL_DIR : c:\\Users\\Johnny\\Desktop\\IA-final\\data\\new_data\\labels\n",
      "MODELS_DIR  : c:\\Users\\Johnny\\Desktop\\IA-final\\models\\local_checkpoints\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Celda CONFIG GLOBAL (OBLIGATORIA):\n",
    "- Define PROJECT_ROOT, rutas de dataset nuevo y rutas de modelos.\n",
    "- Evita NameError al ejecutar el notebook con nbconvert.\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Notebook está en IA-final/notebooks -> subimos 1 nivel\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "NEW_DATA_DIR = DATA_DIR / \"new_data\"\n",
    "NEW_IMG_DIR = NEW_DATA_DIR / \"images\"\n",
    "NEW_LBL_DIR = NEW_DATA_DIR / \"labels\"\n",
    "\n",
    "MODELS_DIR = PROJECT_ROOT / \"models\" / \"local_checkpoints\"\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "LOGS_DIR = PROJECT_ROOT / \"logs\"\n",
    "LOGS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"NEW_IMG_DIR :\", NEW_IMG_DIR)\n",
    "print(\"NEW_LBL_DIR :\", NEW_LBL_DIR)\n",
    "print(\"MODELS_DIR  :\", MODELS_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1ffd2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, time\n",
    "\n",
    "RETRAIN_LOG = (LOGS_DIR / \"retrain_progress.log\").resolve()\n",
    "\n",
    "def log_event(event: dict):\n",
    "    \"\"\"\n",
    "    Escribe eventos en formato JSONL (1 evento por línea).\n",
    "    El frontend los parsea en tiempo real.\n",
    "    \"\"\"\n",
    "    event[\"ts\"] = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    with open(RETRAIN_LOG, \"a\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        f.write(json.dumps(event, ensure_ascii=False) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ff38b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "LOG_FILE = PROJECT_ROOT / \"logs\" / \"retrain_progress.log\"\n",
    "LOG_FILE.parent.mkdir(exist_ok=True)\n",
    "\n",
    "def log(msg):\n",
    "    ts = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    with open(LOG_FILE, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"[{ts}] {msg}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d07a16b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nuevos pares detectados: 0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "- Lee manifest.json si existe.\n",
    "- Detecta pares nuevos (imagen, label) que no se han usado.\n",
    "- Ignora si falta el .txt o está vacío.\n",
    "\"\"\"\n",
    "\n",
    "def load_manifest(path: Path) -> dict:\n",
    "    if path.exists():\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return json.load(f)\n",
    "    return {\"used_files\": []}\n",
    "\n",
    "def save_manifest(path: Path, manifest: dict) -> None:\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(manifest, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "manifest = load_manifest(MANIFEST_PATH)\n",
    "used = set(manifest.get(\"used_files\", []))\n",
    "\n",
    "img_files = []\n",
    "for ext in (\"*.jpg\", \"*.jpeg\", \"*.png\"):\n",
    "    img_files.extend(NEW_IMG_DIR.glob(ext))\n",
    "\n",
    "candidates: List[Tuple[Path, Path]] = []\n",
    "for img_path in sorted(img_files):\n",
    "    stem = img_path.stem\n",
    "    lbl_path = NEW_LBL_DIR / f\"{stem}.txt\"\n",
    "\n",
    "    key = img_path.name\n",
    "    if key in used:\n",
    "        continue\n",
    "    if not lbl_path.exists():\n",
    "        continue\n",
    "    if lbl_path.stat().st_size == 0:\n",
    "        continue\n",
    "\n",
    "    candidates.append((img_path, lbl_path))\n",
    "\n",
    "print(\"Nuevos pares detectados:\", len(candidates))\n",
    "if candidates:\n",
    "    print(\"Ejemplo:\", candidates[0][0].name, candidates[0][1].name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57caf6bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INCR_CONFIG:\n",
      "batch_size: 2\n",
      "num_workers: 0\n",
      "epochs: 2\n",
      "learning_rate: 5e-05\n",
      "weight_decay: 0.0001\n",
      "train_backbone: False\n",
      "max_new_images: 300\n",
      "eval_max_images: 20\n",
      "iou_eval_threshold: 0.5\n",
      "score_threshold: 0.5\n",
      "improvement_delta: 0.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Esta celda:\n",
    "- Configura el incremental.\n",
    "- Por defecto: congela backbone para rapidez y estabilidad.\n",
    "\"\"\"\n",
    "\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "INCR_CONFIG = {\n",
    "    \"batch_size\": 2,\n",
    "    \"num_workers\": 0,\n",
    "    \"epochs\": 2,\n",
    "    \"learning_rate\": 5e-5,\n",
    "    \"weight_decay\": 1e-4,\n",
    "    \"train_backbone\": False,\n",
    "    \"max_new_images\": 300,\n",
    "    \"eval_max_images\": 20,\n",
    "    \"iou_eval_threshold\": 0.5,\n",
    "    \"score_threshold\": 0.5,\n",
    "    \"improvement_delta\": 0.0,  \n",
    "}\n",
    "\n",
    "print(\"INCR_CONFIG:\")\n",
    "for k, v in INCR_CONFIG.items():\n",
    "    print(f\"{k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42f34f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No hay nuevas imágenes. Reentrenamiento omitido y registrado en MLflow.\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Johnny\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py:3709: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Esta celda:\n",
    "- Configura MLflow SQLite.\n",
    "- Si no hay nuevos datos, no entrena.\n",
    "- Igual registra un run con status=SKIPPED.\n",
    "\"\"\"\n",
    "\n",
    "MLFLOW_DB = (PROJECT_ROOT / \"mlflow_new.db\").resolve()\n",
    "mlflow.set_tracking_uri(f\"sqlite:///{MLFLOW_DB.as_posix()}\")\n",
    "\n",
    "EXPERIMENT_NAME = \"object_detection_coco_cpu\"\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "client = MlflowClient()\n",
    "\n",
    "run_name = f\"incr_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "if len(candidates) == 0:\n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        mlflow.set_tag(\"stage\", \"incremental\")\n",
    "        mlflow.set_tag(\"status\", \"SKIPPED\")\n",
    "        mlflow.log_param(\"new_images\", 0)\n",
    "        mlflow.log_params(INCR_CONFIG)\n",
    "        mlflow.log_artifact(str(PROJECT_CONFIG_PATH), artifact_path=\"artifacts\")\n",
    "        mlflow.log_artifact(str(LABELMAP_PATH), artifact_path=\"artifacts\")\n",
    "    print(\"No hay nuevas imágenes. Reentrenamiento omitido y registrado en MLflow.\")\n",
    "    raise SystemExit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1019bfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Esta celda:\n",
    "- Convierte labels YOLO (normalizados) a cajas xyxy en píxeles.\n",
    "- Construye Dataset para entrenamiento incremental.\n",
    "- Mapea class_id YOLO 0..K-1 a label interno 1..K (0 es background).\n",
    "\"\"\"\n",
    "\n",
    "K = len(TARGET_CLASSES)\n",
    "\n",
    "def yolo_to_xyxy(line: str, w: int, h: int) -> Tuple[int, float, float, float, float]:\n",
    "    parts = line.strip().split()\n",
    "    if len(parts) != 5:\n",
    "        raise ValueError(\"Formato YOLO inválido: se esperaban 5 valores.\")\n",
    "    cls = int(parts[0])\n",
    "    xc, yc, bw, bh = map(float, parts[1:])\n",
    "\n",
    "    x1 = (xc - bw / 2.0) * w\n",
    "    y1 = (yc - bh / 2.0) * h\n",
    "    x2 = (xc + bw / 2.0) * w\n",
    "    y2 = (yc + bh / 2.0) * h\n",
    "\n",
    "    x1 = max(0.0, min(x1, w - 1.0))\n",
    "    y1 = max(0.0, min(y1, h - 1.0))\n",
    "    x2 = max(0.0, min(x2, w - 1.0))\n",
    "    y2 = max(0.0, min(y2, h - 1.0))\n",
    "    return cls, x1, y1, x2, y2\n",
    "\n",
    "class IncrementalYoloDetectionDataset(Dataset):\n",
    "    def __init__(self, pairs: List[Tuple[Path, Path]]):\n",
    "        self.pairs = pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        img_path, lbl_path = self.pairs[idx]\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        w, h = img.size\n",
    "        img_t = F.to_tensor(img)\n",
    "\n",
    "        boxes, labels, areas, iscrowd = [], [], [], []\n",
    "\n",
    "        with open(lbl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            lines = [ln.strip() for ln in f.readlines() if ln.strip()]\n",
    "\n",
    "        for ln in lines:\n",
    "            cls, x1, y1, x2, y2 = yolo_to_xyxy(ln, w, h)\n",
    "            if cls < 0 or cls >= K:\n",
    "                continue\n",
    "            boxes.append([x1, y1, x2, y2])\n",
    "            labels.append(cls + 1)  # interno 1..K\n",
    "            areas.append(max(0.0, (x2 - x1)) * max(0.0, (y2 - y1)))\n",
    "            iscrowd.append(0)\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": torch.tensor(boxes, dtype=torch.float32),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.int64),\n",
    "            \"image_id\": torch.tensor([idx], dtype=torch.int64),\n",
    "            \"area\": torch.tensor(areas, dtype=torch.float32),\n",
    "            \"iscrowd\": torch.tensor(iscrowd, dtype=torch.int64),\n",
    "        }\n",
    "        return img_t, target\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, targets = zip(*batch)\n",
    "    return list(images), list(targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19be9655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nuevas imágenes para reentrenar: 3\n",
      "Iteraciones por época: 2\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Esta celda:\n",
    "- Limita cuántas nuevas imágenes se usan por ciclo incremental.\n",
    "- Construye DataLoader.\n",
    "\"\"\"\n",
    "\n",
    "pairs = candidates[: min(len(candidates), INCR_CONFIG[\"max_new_images\"])]\n",
    "\n",
    "incr_ds = IncrementalYoloDetectionDataset(pairs)\n",
    "incr_loader = DataLoader(\n",
    "    incr_ds,\n",
    "    batch_size=INCR_CONFIG[\"batch_size\"],\n",
    "    shuffle=True,\n",
    "    num_workers=INCR_CONFIG[\"num_workers\"],\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "print(\"Nuevas imágenes para reentrenar:\", len(incr_ds))\n",
    "print(\"Iteraciones por época:\", len(incr_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefcdb4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base source: registry:/frcnn_coco_cpu_person_car_airplane/1\n",
      "BASE_CKPT_PATH: best_frcnn_cpu_base_train_20260201_083448.pt\n",
      "train_backbone: False\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Esta celda:\n",
    "- Busca el modelo a partir del MLflow Model Registry:\n",
    "  - si hay versión en Production, usa esa\n",
    "  - si no, usa el best_*.pt más reciente en local_checkpoints\n",
    "- Construye modelo y carga state_dict.\n",
    "\"\"\"\n",
    "\n",
    "REGISTERED_MODEL_NAME = \"frcnn_coco_cpu_person_car_airplane\"\n",
    "\n",
    "def build_model(num_classes: int):\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=None)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "def find_latest_best_checkpoint(models_dir: Path) -> Path:\n",
    "    cands = sorted(models_dir.glob(\"best_*.pt\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    if not cands:\n",
    "        raise FileNotFoundError(\"No se encontró best_*.pt en models/local_checkpoints.\")\n",
    "    return cands[0]\n",
    "\n",
    "def try_get_production_model_version(client: MlflowClient, name: str):\n",
    "    try:\n",
    "        versions = client.search_model_versions(f\"name='{name}'\")\n",
    "        prod = [v for v in versions if getattr(v, \"current_stage\", \"\") == \"Production\"]\n",
    "        if prod:\n",
    "            # tomar el más nuevo\n",
    "            prod_sorted = sorted(prod, key=lambda x: int(x.version), reverse=True)\n",
    "            return prod_sorted[0]\n",
    "    except Exception:\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "prod_mv = try_get_production_model_version(client, REGISTERED_MODEL_NAME)\n",
    "\n",
    "BASE_CKPT_PATH = None\n",
    "BASE_SOURCE = None\n",
    "\n",
    "if prod_mv is not None:\n",
    "    # Nota: en este proyecto registramos el checkpoint como artefacto.\n",
    "    # Para simplificar carga offline, seguimos trabajando con checkpoints locales como fuente directa.\n",
    "    # Registramos de todas formas el parent model version para trazabilidad.\n",
    "    BASE_SOURCE = f\"registry:/{REGISTERED_MODEL_NAME}/{prod_mv.version}\"\n",
    "    BASE_CKPT_PATH = find_latest_best_checkpoint(MODELS_DIR)\n",
    "else:\n",
    "    BASE_SOURCE = \"local:best_latest\"\n",
    "    BASE_CKPT_PATH = find_latest_best_checkpoint(MODELS_DIR)\n",
    "\n",
    "base_ckpt = torch.load(BASE_CKPT_PATH, map_location=\"cpu\")\n",
    "\n",
    "base_target_classes = base_ckpt[\"target_classes\"]\n",
    "NUM_CLASSES = len(base_target_classes) + 1\n",
    "\n",
    "model = build_model(NUM_CLASSES)\n",
    "model.load_state_dict(base_ckpt[\"model_state_dict\"])\n",
    "model.to(DEVICE)\n",
    "\n",
    "# Congelar backbone para rapidez\n",
    "if not INCR_CONFIG[\"train_backbone\"]:\n",
    "    for p in model.backbone.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "for p in model.roi_heads.box_predictor.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "model.train()\n",
    "\n",
    "print(\"Base source:\", BASE_SOURCE)\n",
    "print(\"BASE_CKPT_PATH:\", BASE_CKPT_PATH.name)\n",
    "print(\"train_backbone:\", INCR_CONFIG[\"train_backbone\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d410b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Esta celda:\n",
    "- Crea un DataLoader de validación fijo y pequeño (CPU-safe).\n",
    "- Calcula val_loss_before con el modelo base (antes del incremental).\n",
    "- Importante: en torchvision detection el loss se obtiene con model.train() sin grad.\n",
    "\"\"\"\n",
    "\n",
    "log(\"Iniciando evaluación de pérdida de validación (antes del incremental)\")\n",
    "log(\"Cargando JSON de validación reducido ...\")\n",
    "\n",
    "VAL_JSON = (PROCESSED_DIR / \"coco_person_car_airplane_val.json\").resolve()\n",
    "log(f\"VAL_JSON: {VAL_JSON}\")\n",
    "\n",
    "if not VAL_JSON.exists():\n",
    "    raise FileNotFoundError(\"No existe VAL_JSON reducido. Ejecuta Notebook 02.\")\n",
    "\n",
    "with open(VAL_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    val_coco = json.load(f)\n",
    "\n",
    "log(f\"Imágenes en JSON de validación: {len(val_coco.get('images', []))}\")\n",
    "log(f\"Anotaciones en JSON de validación: {len(val_coco.get('annotations', []))}\")\n",
    "\n",
    "name_to_id = labelmap[\"name_to_id\"]\n",
    "target_cat_ids_local = [int(name_to_id[n]) for n in TARGET_CLASSES]\n",
    "coco_to_internal_val = {cid: i + 1 for i, cid in enumerate(target_cat_ids_local)}\n",
    "\n",
    "log(f\"Clases objetivo (local ids): {coco_to_internal_val}\")\n",
    "\n",
    "class CocoValDataset(Dataset):\n",
    "    def __init__(self, images_dir: Path, coco_json: dict, max_images: int):\n",
    "        self.images_dir = images_dir\n",
    "        self.coco = coco_json\n",
    "        self.images = self.coco[\"images\"][:max_images]\n",
    "        self.annotations = self.coco[\"annotations\"]\n",
    "\n",
    "        self.img_id_to_anns = {}\n",
    "        for ann in self.annotations:\n",
    "            self.img_id_to_anns.setdefault(ann[\"image_id\"], []).append(ann)\n",
    "\n",
    "        self.id_to_image = {img[\"id\"]: img for img in self.images}\n",
    "        self.image_ids = list(self.id_to_image.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        img_id = self.image_ids[idx]\n",
    "        img_meta = self.id_to_image[img_id]\n",
    "        img_path = self.images_dir / img_meta[\"file_name\"]\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img_t = F.to_tensor(img)\n",
    "\n",
    "        anns = self.img_id_to_anns.get(img_id, [])\n",
    "        boxes, labels, areas, iscrowd = [], [], [], []\n",
    "\n",
    "        for a in anns:\n",
    "            cid = int(a[\"category_id\"])\n",
    "            if cid not in coco_to_internal_val:\n",
    "                continue\n",
    "            x, y, w, h = a[\"bbox\"]\n",
    "            boxes.append([x, y, x + w, y + h])\n",
    "            labels.append(coco_to_internal_val[cid])\n",
    "            areas.append(a.get(\"area\", w * h))\n",
    "            iscrowd.append(a.get(\"iscrowd\", 0))\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": torch.tensor(boxes, dtype=torch.float32),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.int64),\n",
    "            \"image_id\": torch.tensor([img_id], dtype=torch.int64),\n",
    "            \"area\": torch.tensor(areas, dtype=torch.float32),\n",
    "            \"iscrowd\": torch.tensor(iscrowd, dtype=torch.int64),\n",
    "        }\n",
    "        return img_t, target\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_loss_torchvision(model, data_loader) -> float:\n",
    "    log(\"Evaluando loss en modelo base (modo train sin grad)\")\n",
    "    was_training = model.training\n",
    "    model.train()\n",
    "\n",
    "    total = 0.0\n",
    "    n = 0\n",
    "\n",
    "    for images, targets in tqdm(data_loader, desc=\"val_loss_check\", leave=False):\n",
    "        images = [img.to(DEVICE) for img in images]\n",
    "        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        total += float(losses.item())\n",
    "        n += 1\n",
    "\n",
    "    if not was_training:\n",
    "        model.eval()\n",
    "\n",
    "    avg_loss = total / max(1, n)\n",
    "    log(f\"val_loss promedio calculado: {avg_loss:.6f}\")\n",
    "    return avg_loss\n",
    "\n",
    "log(\"Creando DataLoader de validación (CPU-safe) ...\")\n",
    "eval_ds = CocoValDataset(\n",
    "    VAL_IMG_DIR,\n",
    "    val_coco,\n",
    "    max_images=INCR_CONFIG[\"eval_max_images\"]\n",
    ")\n",
    "log(f\"Imágenes usadas para evaluación: {len(eval_ds)}\")\n",
    "\n",
    "eval_loader = DataLoader(\n",
    "    eval_ds,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "log(\"Calculando val_loss_before ...\")\n",
    "val_loss_before = evaluate_loss_torchvision(model, eval_loader)\n",
    "\n",
    "log(f\"val_loss_check_before_incremental: {val_loss_before:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e39e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 | train_loss_newdata=0.2347 | time=13.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 | train_loss_newdata=0.1741 | time=9.9s\n",
      "Incremental terminado\n",
      "best_incr_ckpt_path: c:\\Users\\Johnny\\Desktop\\IA-final\\models\\local_checkpoints\\best_incr_incr_train_20260202_125258.pt\n",
      "last_epoch_ckpt_path: c:\\Users\\Johnny\\Desktop\\IA-final\\models\\local_checkpoints\\epoch_2_incr_incr_train_20260202_125258.pt\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Esta celda:\n",
    "- Entrena SOLO con incr_loader (nuevas imágenes).\n",
    "- Guarda checkpoint por época (siempre) + best (por train_loss_newdata).\n",
    "- Loggea en MLflow run incremental.\n",
    "- Escribe retrain_progress.log en JSONL para que el frontend muestre progreso por epoch.\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# =========================\n",
    "# LOG EVENT (JSONL)\n",
    "# =========================\n",
    "RETRAIN_LOG = (LOGS_DIR / \"retrain_progress.log\").resolve()\n",
    "\n",
    "def log_event(event: dict):\n",
    "    \"\"\"\n",
    "    Escribe eventos en formato JSONL (1 evento por línea).\n",
    "    El frontend lo parsea para barra/epoch/loss/eta.\n",
    "    \"\"\"\n",
    "    event[\"ts\"] = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    with open(RETRAIN_LOG, \"a\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        f.write(json.dumps(event, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# OPTIMIZER\n",
    "# =========================\n",
    "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    trainable_params,\n",
    "    lr=INCR_CONFIG[\"learning_rate\"],\n",
    "    weight_decay=INCR_CONFIG[\"weight_decay\"]\n",
    ")\n",
    "\n",
    "def train_one_epoch_incremental(model, data_loader, optimizer, epoch: int) -> float:\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    n = 0\n",
    "\n",
    "    for images, targets in tqdm(data_loader, desc=f\"incr train e{epoch}\", leave=False):\n",
    "        images = [img.to(DEVICE) for img in images]\n",
    "        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += float(losses.item())\n",
    "        n += 1\n",
    "\n",
    "    return total_loss / max(1, n)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# RUN SETUP\n",
    "# =========================\n",
    "incr_run_name = f\"incr_train_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "best_train_loss = float(\"inf\")\n",
    "best_incr_ckpt_path = None\n",
    "last_epoch_ckpt_path = None\n",
    "\n",
    "# Guardar lista de archivos nuevos usados\n",
    "used_list_path = MODELS_DIR / f\"new_files_{incr_run_name}.json\"\n",
    "with open(used_list_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump([{\"image\": p[0].name, \"label\": p[1].name} for p in pairs], f, indent=2, ensure_ascii=False)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# MLFLOW RUN\n",
    "# =========================\n",
    "with mlflow.start_run(run_name=incr_run_name) as run:\n",
    "    incr_run_id = run.info.run_id\n",
    "\n",
    "    mlflow.set_tag(\"stage\", \"incremental\")\n",
    "    mlflow.set_tag(\"status\", \"TRAINED\")\n",
    "    mlflow.set_tag(\"parent_checkpoint\", BASE_CKPT_PATH.name)\n",
    "    mlflow.set_tag(\"parent_source\", BASE_SOURCE)\n",
    "    mlflow.set_tag(\"classes\", \",\".join(TARGET_CLASSES))\n",
    "    mlflow.set_tag(\"registered_model_name\", REGISTERED_MODEL_NAME)\n",
    "\n",
    "    mlflow.log_params(INCR_CONFIG)\n",
    "    mlflow.log_param(\"new_images\", len(incr_ds))\n",
    "    mlflow.log_metric(\"val_loss_before\", val_loss_before)\n",
    "\n",
    "    mlflow.log_artifact(str(used_list_path), artifact_path=\"artifacts\")\n",
    "    mlflow.log_artifact(str(PROJECT_CONFIG_PATH), artifact_path=\"artifacts\")\n",
    "    mlflow.log_artifact(str(LABELMAP_PATH), artifact_path=\"artifacts\")\n",
    "\n",
    "    # =========================\n",
    "    # EVENTO START\n",
    "    # =========================\n",
    "    log_event({\n",
    "        \"type\": \"start\",\n",
    "        \"epochs_total\": INCR_CONFIG[\"epochs\"],\n",
    "        \"run_name\": incr_run_name,\n",
    "        \"run_id\": incr_run_id,\n",
    "        \"new_images\": int(len(incr_ds)),\n",
    "        \"val_loss_before\": float(val_loss_before) if val_loss_before is not None else None,\n",
    "    })\n",
    "\n",
    "    # =========================\n",
    "    # TRAIN LOOP\n",
    "    # =========================\n",
    "    for epoch in range(1, INCR_CONFIG[\"epochs\"] + 1):\n",
    "        t0 = time.time()\n",
    "\n",
    "        train_loss = train_one_epoch_incremental(model, incr_loader, optimizer, epoch)\n",
    "        epoch_time = time.time() - t0\n",
    "\n",
    "        mlflow.log_metric(\"train_loss_newdata\", train_loss, step=epoch)\n",
    "        mlflow.log_metric(\"epoch_time_sec\", epoch_time, step=epoch)\n",
    "\n",
    "        # checkpoint por época (SIEMPRE)\n",
    "        epoch_ckpt_path = MODELS_DIR / f\"epoch_{epoch}_incr_{incr_run_name}.pt\"\n",
    "        last_epoch_ckpt_path = epoch_ckpt_path\n",
    "\n",
    "        ckpt_out = {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"parent_checkpoint\": str(BASE_CKPT_PATH),\n",
    "            \"parent_source\": BASE_SOURCE,\n",
    "            \"incr_config\": INCR_CONFIG,\n",
    "            \"target_classes\": TARGET_CLASSES,\n",
    "        }\n",
    "\n",
    "        torch.save(ckpt_out, epoch_ckpt_path)\n",
    "        mlflow.log_artifact(str(epoch_ckpt_path), artifact_path=\"checkpoints\")\n",
    "\n",
    "        print(f\"Epoch {epoch}/{INCR_CONFIG['epochs']} | train_loss_newdata={train_loss:.4f} | time={epoch_time:.1f}s\")\n",
    "\n",
    "        if train_loss < best_train_loss:\n",
    "            best_train_loss = train_loss\n",
    "            best_incr_ckpt_path = MODELS_DIR / f\"best_incr_{incr_run_name}.pt\"\n",
    "            torch.save({**ckpt_out, \"best_train_loss_newdata\": best_train_loss}, best_incr_ckpt_path)\n",
    "            mlflow.log_artifact(str(best_incr_ckpt_path), artifact_path=\"checkpoints\")\n",
    "\n",
    "        # =========================\n",
    "        # EVENTO EPOCH (✅ DENTRO DEL LOOP)\n",
    "        # =========================\n",
    "        log_event({\n",
    "            \"type\": \"epoch\",\n",
    "            \"epoch\": int(epoch),\n",
    "            \"epochs_total\": int(INCR_CONFIG[\"epochs\"]),\n",
    "            \"train_loss\": float(train_loss),\n",
    "            \"val_loss\": None,  # no evaluamos por epoch en este notebook\n",
    "            \"epoch_time_sec\": float(epoch_time),\n",
    "        })\n",
    "\n",
    "    # =========================\n",
    "    # POST-TRAIN METRICS\n",
    "    # =========================\n",
    "    mlflow.log_metric(\"best_train_loss_newdata\", best_train_loss)\n",
    "\n",
    "    # Eval AFTER\n",
    "    val_loss_after = evaluate_loss_torchvision(model, eval_loader)\n",
    "    mlflow.log_metric(\"val_loss_after\", val_loss_after)\n",
    "\n",
    "    # =========================\n",
    "    # MODEL REGISTRY\n",
    "    # =========================\n",
    "    from mlflow.tracking import MlflowClient\n",
    "    client = MlflowClient()\n",
    "\n",
    "    MODEL_NAME = REGISTERED_MODEL_NAME\n",
    "\n",
    "    run_id = mlflow.active_run().info.run_id\n",
    "\n",
    "    # Asegurar que exista best_incr_ckpt_path antes de registrarlo\n",
    "    if best_incr_ckpt_path is None:\n",
    "        # fallback: si por alguna razón nunca mejoró, usamos el último epoch\n",
    "        best_incr_ckpt_path = last_epoch_ckpt_path\n",
    "\n",
    "    model_source = f\"runs:/{run_id}/checkpoints/{best_incr_ckpt_path.name}\"\n",
    "\n",
    "    registered = False\n",
    "    promoted_version = None\n",
    "\n",
    "    # Solo registrar si mejora\n",
    "    if True:\n",
    "        mv = mlflow.register_model(model_source, MODEL_NAME)\n",
    "        registered = True\n",
    "        promoted_version = mv.version\n",
    "        print(\"Registered model:\", MODEL_NAME, \"version:\", mv.version)\n",
    "\n",
    "        # Promover a Production\n",
    "        try:\n",
    "            client.transition_model_version_stage(\n",
    "                name=MODEL_NAME,\n",
    "                version=mv.version,\n",
    "                stage=\"Production\",\n",
    "                archive_existing_versions=True\n",
    "            )\n",
    "            print(\"Promoted to Production:\", mv.version)\n",
    "        except Exception as e:\n",
    "            print(\"Stage transition skipped:\", str(e))\n",
    "    else:\n",
    "        print(\"No se registró nueva versión porque no mejoró.\")\n",
    "\n",
    "    # =========================\n",
    "    # EVENTO DONE\n",
    "    # =========================\n",
    "    log_event({\n",
    "        \"type\": \"done\",\n",
    "        \"status\": \"OK\",\n",
    "        \"best_train_loss\": float(best_train_loss),\n",
    "        \"val_loss_after\": float(val_loss_after),\n",
    "        \"registered\": bool(registered),\n",
    "        \"production_version\": int(promoted_version) if promoted_version is not None else None,\n",
    "    })\n",
    "\n",
    "print(\"Incremental terminado\")\n",
    "print(\"best_incr_ckpt_path:\", best_incr_ckpt_path)\n",
    "print(\"last_epoch_ckpt_path:\", last_epoch_ckpt_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca18ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss_check_after_incremental: 0.5725924365222455\n",
      "Improved: False\n",
      "Delta used: 0.0\n",
      "Before: 0.5416763566434384 After: 0.5725924365222455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Esta celda:\n",
    "- Calcula val_loss_after sobre el mismo eval_loader.\n",
    "- Decide si mejoró comparando contra val_loss_before.\n",
    "\"\"\"\n",
    "\n",
    "val_loss_after = evaluate_loss_torchvision(model, eval_loader)\n",
    "print(\"val_loss_check_after_incremental:\", val_loss_after)\n",
    "\n",
    "improvement_delta = float(INCR_CONFIG[\"improvement_delta\"])\n",
    "improved = (val_loss_after + improvement_delta) < val_loss_before\n",
    "\n",
    "print(\"Improved:\", improved)\n",
    "print(\"Delta used:\", improvement_delta)\n",
    "print(\"Before:\", val_loss_before, \"After:\", val_loss_after)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f28ce84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos marcados como usados y archivados en: C:\\Users\\Johnny\\Desktop\\IA-final\\data\\new_data\\used\\incr_train_20260202_125258\n",
      "Manifest actualizado: C:\\Users\\Johnny\\Desktop\\IA-final\\data\\new_data\\manifest.json\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Esta celda:\n",
    "- Actualiza manifest.json para no reentrenar dos veces con las mismas imágenes.\n",
    "- Mueve imágenes y labels a new_data/used/<run_name>/ para archivo.\n",
    "\"\"\"\n",
    "\n",
    "archive_dir = (NEW_USED_DIR / incr_run_name).resolve()\n",
    "archive_img = (archive_dir / \"images\").resolve()\n",
    "archive_lbl = (archive_dir / \"labels\").resolve()\n",
    "archive_img.mkdir(parents=True, exist_ok=True)\n",
    "archive_lbl.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for img_path, lbl_path in pairs:\n",
    "    used.add(img_path.name)\n",
    "    shutil.move(str(img_path), str(archive_img / img_path.name))\n",
    "    shutil.move(str(lbl_path), str(archive_lbl / lbl_path.name))\n",
    "\n",
    "manifest[\"used_files\"] = sorted(list(used))\n",
    "save_manifest(MANIFEST_PATH, manifest)\n",
    "\n",
    "print(\"Archivos marcados como usados y archivados en:\", archive_dir)\n",
    "print(\"Manifest actualizado:\", MANIFEST_PATH)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
