{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf769173",
   "metadata": {},
   "source": [
    "# Fifth Notebook\n",
    "\n",
    "In this final notebook, incremental retraining is performed using only newly labeled images, executed quickly on CPU, with per-epoch checkpoint saving and MLflow (SQLite) logging.  \n",
    "If no new data is available, an MLflow run is created with status SKIPPED and no training is performed.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "1. Read newly labeled user images from `data/new_data/` (YOLO format).\n",
    "2. Train ONLY on those new images (fast CPU training).\n",
    "3. Objectively evaluate improvement using a fixed validation subset (val_loss_before vs val_loss_after).\n",
    "4. Log in MLflow:\n",
    "   - incremental run\n",
    "   - new model version in the Model Registry\n",
    "   - improvement tags\n",
    "5. If improvement is observed, automatically promote the model to Production (archiving previous versions).\n",
    "6. If no new data is found, skip training and log a SKIPPED run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2befeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- Imports the necessary libraries for incremental retraining, evaluation, and MLflow Registry.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from mlflow.tracking import MlflowClient\n",
    "import time\n",
    "\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.transforms import functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137fe92e",
   "metadata": {},
   "source": [
    "1. Carga de carpetas previas para el fomrat establecido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa235321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: C:\\Users\\Johnny\\Desktop\\IA-final\n",
      "TARGET_CLASSES: ['person', 'car', 'airplane']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "- Locates PROJECT_ROOT based on data/processed/project_config.json.\n",
    "- Loads project_config.json and labelmap.json.\n",
    "- Defines the project’s standard paths.\n",
    "\"\"\"\n",
    "\n",
    "def find_project_root(start: Path, max_up: int = 8) -> Path:\n",
    "    cur = start.resolve()\n",
    "    for _ in range(max_up):\n",
    "        if (cur / \"data\" / \"processed\" / \"project_config.json\").exists():\n",
    "            return cur\n",
    "        cur = cur.parent\n",
    "    raise FileNotFoundError(\"No se encontró data/processed/project_config.json. Ejecuta Notebook 01.\")\n",
    "\n",
    "PROJECT_ROOT = find_project_root(Path.cwd())\n",
    "PROCESSED_DIR = (PROJECT_ROOT / \"data\" / \"processed\").resolve()\n",
    "\n",
    "PROJECT_CONFIG_PATH = (PROCESSED_DIR / \"project_config.json\").resolve()\n",
    "LABELMAP_PATH = (PROCESSED_DIR / \"labelmap.json\").resolve()\n",
    "\n",
    "with open(PROJECT_CONFIG_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    project_config = json.load(f)\n",
    "\n",
    "with open(LABELMAP_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    labelmap = json.load(f)\n",
    "\n",
    "VAL_IMG_DIR = Path(project_config[\"val_dir\"])\n",
    "TARGET_CLASSES = project_config[\"target_classes\"]\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"TARGET_CLASSES:\", TARGET_CLASSES)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a89398",
   "metadata": {},
   "source": [
    "2. Nuevas carpetas para el manjeo de datos proveniente de la interfaz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204f4941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NEW_IMG_DIR: C:\\Users\\Johnny\\Desktop\\IA-final\\data\\new_data\\images\n",
      "NEW_LBL_DIR: C:\\Users\\Johnny\\Desktop\\IA-final\\data\\new_data\\labels\n",
      "NEW_USED_DIR: C:\\Users\\Johnny\\Desktop\\IA-final\\data\\new_data\\used\n",
      "MANIFEST_PATH: C:\\Users\\Johnny\\Desktop\\IA-final\\data\\new_data\\manifest.json\n",
      "\n",
      "Formato YOLO esperado por label:\n",
      "class_id x_center y_center width height  (todo normalizado 0..1)\n",
      "class_id: 0=person, 1=car, 2=airplane\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "- Defines the structure for new data:\n",
    "  - data/new_data/images/*.jpg|png\n",
    "  - data/new_data/labels/*.txt (YOLO format), same filename as the image\n",
    "  - data/new_data/used/ (records of already used files)\n",
    "  - data/new_data/manifest.json (to avoid duplicate processing)\n",
    "\"\"\"\n",
    "\n",
    "NEW_DATA_DIR = (PROJECT_ROOT / \"data\" / \"new_data\").resolve()\n",
    "NEW_IMG_DIR = (NEW_DATA_DIR / \"images\").resolve()\n",
    "NEW_LBL_DIR = (NEW_DATA_DIR / \"labels\").resolve()\n",
    "NEW_USED_DIR = (NEW_DATA_DIR / \"used\").resolve()\n",
    "MANIFEST_PATH = (NEW_DATA_DIR / \"manifest.json\").resolve()\n",
    "\n",
    "for d in [NEW_DATA_DIR, NEW_IMG_DIR, NEW_LBL_DIR, NEW_USED_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"NEW_IMG_DIR:\", NEW_IMG_DIR)\n",
    "print(\"NEW_LBL_DIR:\", NEW_LBL_DIR)\n",
    "print(\"NEW_USED_DIR:\", NEW_USED_DIR)\n",
    "print(\"MANIFEST_PATH:\", MANIFEST_PATH)\n",
    "\n",
    "print(\"\\nFormato YOLO esperado por label:\")\n",
    "print(\"class_id x_center y_center width height  (todo normalizado 0..1)\")\n",
    "print(\"class_id: 0=person, 1=car, 2=airplane\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c21db7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: c:\\Users\\Johnny\\Desktop\\IA-final\n",
      "NEW_IMG_DIR : c:\\Users\\Johnny\\Desktop\\IA-final\\data\\new_data\\images\n",
      "NEW_LBL_DIR : c:\\Users\\Johnny\\Desktop\\IA-final\\data\\new_data\\labels\n",
      "MODELS_DIR  : c:\\Users\\Johnny\\Desktop\\IA-final\\models\\local_checkpoints\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "GLOBAL CONFIG cell (MANDATORY):\n",
    "- Defines PROJECT_ROOT, new dataset paths, and model paths.\n",
    "- Prevents NameError when executing the notebook with nbconvert.\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Notebook está en IA-final/notebooks -> subimos 1 nivel\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "\n",
    "DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "NEW_DATA_DIR = DATA_DIR / \"new_data\"\n",
    "NEW_IMG_DIR = NEW_DATA_DIR / \"images\"\n",
    "NEW_LBL_DIR = NEW_DATA_DIR / \"labels\"\n",
    "\n",
    "MODELS_DIR = PROJECT_ROOT / \"models\" / \"local_checkpoints\"\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "LOGS_DIR = PROJECT_ROOT / \"logs\"\n",
    "LOGS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"NEW_IMG_DIR :\", NEW_IMG_DIR)\n",
    "print(\"NEW_LBL_DIR :\", NEW_LBL_DIR)\n",
    "print(\"MODELS_DIR  :\", MODELS_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ffd2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, time\n",
    "\"\"\"\n",
    "Writes events in JSONL format (one event per line).\n",
    "The frontend parses them in real time.\n",
    "\"\"\"\n",
    "\n",
    "RETRAIN_LOG = (LOGS_DIR / \"retrain_progress.log\").resolve()\n",
    "\n",
    "def log_event(event: dict):\n",
    "\n",
    "    event[\"ts\"] = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    with open(RETRAIN_LOG, \"a\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        f.write(json.dumps(event, ensure_ascii=False) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff38b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import time\n",
    "\"\"\"\n",
    "Simple logging setup for incremental retraining.\n",
    "\n",
    "- Defines PROJECT_ROOT and creates a logs directory if it does not exist.\n",
    "- LOG_FILE stores progress messages in append mode.\n",
    "- The log() function writes timestamped messages to help track execution.\n",
    "\"\"\"\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "LOG_FILE = PROJECT_ROOT / \"logs\" / \"retrain_progress.log\"\n",
    "LOG_FILE.parent.mkdir(exist_ok=True)\n",
    "\n",
    "def log(msg):\n",
    "    ts = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    with open(LOG_FILE, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"[{ts}] {msg}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07a16b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pares válidos totales en new_data: 7\n",
      "Nuevos pares detectados: 7\n",
      "Ejemplo nuevo: imagen_20260204_121828_imagen_2026-02-04_120732568.png imagen_20260204_121828_imagen_2026-02-04_120732568.txt\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "- Reads manifest.json if it exists.\n",
    "- Detects new (image, label) pairs that have not been used before.\n",
    "- Ignores files if the corresponding .txt label is missing or empty.\n",
    "\"\"\"\n",
    "\n",
    "def load_manifest(path: Path) -> dict:\n",
    "    if path.exists():\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return json.load(f)\n",
    "    return {\"used_files\": []}\n",
    "\n",
    "def save_manifest(path: Path, manifest: dict) -> None:\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(manifest, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "manifest = load_manifest(MANIFEST_PATH)\n",
    "used = set(manifest.get(\"used_files\", []))\n",
    "\n",
    "img_files = []\n",
    "for ext in (\"*.jpg\", \"*.jpeg\", \"*.png\"):\n",
    "    img_files.extend(NEW_IMG_DIR.glob(ext))\n",
    "\n",
    "all_pairs: List[Tuple[Path, Path]] = []\n",
    "new_pairs: List[Tuple[Path, Path]] = []\n",
    "\n",
    "for img_path in sorted(img_files):\n",
    "    stem = img_path.stem\n",
    "    lbl_path = NEW_LBL_DIR / f\"{stem}.txt\"\n",
    "\n",
    "    # válido = label existe y no está vacío\n",
    "    if (not lbl_path.exists()) or (lbl_path.stat().st_size == 0):\n",
    "        continue\n",
    "\n",
    "    all_pairs.append((img_path, lbl_path))\n",
    "\n",
    "    key = img_path.name\n",
    "    if key not in used:\n",
    "        new_pairs.append((img_path, lbl_path))\n",
    "\n",
    "print(\"Pares válidos totales en new_data:\", len(all_pairs))\n",
    "print(\"Nuevos pares detectados:\", len(new_pairs))\n",
    "if new_pairs:\n",
    "    print(\"Ejemplo nuevo:\", new_pairs[0][0].name, new_pairs[0][1].name)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57caf6bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INCR_CONFIG:\n",
      "batch_size: 2\n",
      "num_workers: 0\n",
      "epochs: 2\n",
      "learning_rate: 5e-05\n",
      "weight_decay: 0.0001\n",
      "train_backbone: False\n",
      "max_new_images: 300\n",
      "eval_max_images: 20\n",
      "iou_eval_threshold: 0.5\n",
      "score_threshold: 0.5\n",
      "improvement_delta: 0.0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This cell:\n",
    "- Configures the incremental training process.\n",
    "- By default, freezes the backbone for faster and more stable retraining.\n",
    "\"\"\"\n",
    "\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "INCR_CONFIG = {\n",
    "    \"batch_size\": 2,\n",
    "    \"num_workers\": 0,\n",
    "    \"epochs\": 2,\n",
    "    \"learning_rate\": 5e-5,\n",
    "    \"weight_decay\": 1e-4,\n",
    "    \"train_backbone\": False,\n",
    "    \"max_new_images\": 300,\n",
    "    \"eval_max_images\": 20,\n",
    "    \"iou_eval_threshold\": 0.5,\n",
    "    \"score_threshold\": 0.5,\n",
    "    \"improvement_delta\": 0.0,  \n",
    "}\n",
    "\n",
    "print(\"INCR_CONFIG:\")\n",
    "for k, v in INCR_CONFIG.items():\n",
    "    print(f\"{k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f34f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell:\n",
    "- Configures MLflow with SQLite.\n",
    "- If NO new images are found:\n",
    "  - Logs a run with status=SKIPPED\n",
    "  - BUT does not stop the training process\n",
    "- Training will continue using pairs_train (entire new_data set).\n",
    "\"\"\"\n",
    "\n",
    "MLFLOW_DB = (PROJECT_ROOT / \"mlflow_new.db\").resolve()\n",
    "mlflow.set_tracking_uri(f\"sqlite:///{MLFLOW_DB.as_posix()}\")\n",
    "\n",
    "EXPERIMENT_NAME = \"object_detection_coco_cpu\"\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "client = MlflowClient()\n",
    "\n",
    "run_name = f\"incr_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "# ============================================================\n",
    "# CASO: NO HAY NUEVAS IMÁGENES\n",
    "# ============================================================\n",
    "if len(new_pairs) == 0:\n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        mlflow.set_tag(\"stage\", \"incremental\")\n",
    "        mlflow.set_tag(\"status\", \"SKIPPED\")\n",
    "        mlflow.log_param(\"new_images\", 0)\n",
    "        mlflow.log_param(\"train_images_total\", len(pairs_train))\n",
    "        mlflow.log_params(INCR_CONFIG)\n",
    "        mlflow.log_artifact(str(PROJECT_CONFIG_PATH), artifact_path=\"artifacts\")\n",
    "        mlflow.log_artifact(str(LABELMAP_PATH), artifact_path=\"artifacts\")\n",
    "\n",
    "    print(\n",
    "        \"No hay nuevas imágenes, \"\n",
    "        \"pero se continuará el entrenamiento con new_data existente.\"\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1019bfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This cell:\n",
    "- Converts YOLO labels (normalized) into pixel-space xyxy bounding boxes.\n",
    "- Builds the Dataset for incremental training.\n",
    "- Maps YOLO class_id 0..K-1 to internal labels 1..K (0 is reserved for background).\n",
    "\"\"\"\n",
    "\n",
    "K = len(TARGET_CLASSES)\n",
    "\n",
    "def yolo_to_xyxy(line: str, w: int, h: int) -> Tuple[int, float, float, float, float]:\n",
    "    parts = line.strip().split()\n",
    "    if len(parts) != 5:\n",
    "        raise ValueError(\"Formato YOLO inválido: se esperaban 5 valores.\")\n",
    "    cls = int(parts[0])\n",
    "    xc, yc, bw, bh = map(float, parts[1:])\n",
    "\n",
    "    x1 = (xc - bw / 2.0) * w\n",
    "    y1 = (yc - bh / 2.0) * h\n",
    "    x2 = (xc + bw / 2.0) * w\n",
    "    y2 = (yc + bh / 2.0) * h\n",
    "\n",
    "    x1 = max(0.0, min(x1, w - 1.0))\n",
    "    y1 = max(0.0, min(y1, h - 1.0))\n",
    "    x2 = max(0.0, min(x2, w - 1.0))\n",
    "    y2 = max(0.0, min(y2, h - 1.0))\n",
    "    return cls, x1, y1, x2, y2\n",
    "\n",
    "class IncrementalYoloDetectionDataset(Dataset):\n",
    "    def __init__(self, pairs: List[Tuple[Path, Path]]):\n",
    "        self.pairs = pairs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        img_path, lbl_path = self.pairs[idx]\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        w, h = img.size\n",
    "        img_t = F.to_tensor(img)\n",
    "\n",
    "        boxes, labels, areas, iscrowd = [], [], [], []\n",
    "\n",
    "        with open(lbl_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            lines = [ln.strip() for ln in f.readlines() if ln.strip()]\n",
    "\n",
    "        for ln in lines:\n",
    "            cls, x1, y1, x2, y2 = yolo_to_xyxy(ln, w, h)\n",
    "            if cls < 0 or cls >= K:\n",
    "                continue\n",
    "            boxes.append([x1, y1, x2, y2])\n",
    "            labels.append(cls + 1)  # interno 1..K\n",
    "            areas.append(max(0.0, (x2 - x1)) * max(0.0, (y2 - y1)))\n",
    "            iscrowd.append(0)\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": torch.tensor(boxes, dtype=torch.float32),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.int64),\n",
    "            \"image_id\": torch.tensor([idx], dtype=torch.int64),\n",
    "            \"area\": torch.tensor(areas, dtype=torch.float32),\n",
    "            \"iscrowd\": torch.tensor(iscrowd, dtype=torch.int64),\n",
    "        }\n",
    "        return img_t, target\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, targets = zip(*batch)\n",
    "    return list(images), list(targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "19be9655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nuevas imágenes detectadas (para reporte): 7\n",
      "Total imágenes usadas para ENTRENAR (new_data completo): 7\n",
      "Iteraciones por época: 4\n"
     ]
    }
   ],
   "source": [
    "# Para mostrar \"nuevas\"\n",
    "pairs_new = new_pairs[: min(len(new_pairs), INCR_CONFIG[\"max_new_images\"])]\n",
    "\n",
    "# Para ENTRENAR: TODO lo que hay en new_data (válido)\n",
    "pairs_train = all_pairs  # <- aquí está el cambio clave\n",
    "\n",
    "incr_ds = IncrementalYoloDetectionDataset(pairs_train)\n",
    "incr_loader = DataLoader(\n",
    "    incr_ds,\n",
    "    batch_size=INCR_CONFIG[\"batch_size\"],\n",
    "    shuffle=True,\n",
    "    num_workers=INCR_CONFIG[\"num_workers\"],\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "print(\"Nuevas imágenes detectadas (para reporte):\", len(pairs_new))\n",
    "print(\"Total imágenes usadas para ENTRENAR (new_data completo):\", len(pairs_train))\n",
    "print(\"Iteraciones por época:\", len(incr_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefcdb4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base source: registry:/frcnn_coco_cpu_person_car_airplane/13\n",
      "BASE_CKPT_PATH: best_incr_incr_train_20260204_121034.pt\n",
      "train_backbone: False\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This cell:\n",
    "- Loads the model from the MLflow Model Registry:\n",
    "  - If a Production version exists, it uses that one.\n",
    "  - Otherwise, it falls back to the most recent best_*.pt in local_checkpoints.\n",
    "- Rebuilds the model architecture and loads the corresponding state_dict.\n",
    "\"\"\"\n",
    "\n",
    "REGISTERED_MODEL_NAME = \"frcnn_coco_cpu_person_car_airplane\"\n",
    "\n",
    "def build_model(num_classes: int):\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=None)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "def find_latest_best_checkpoint(models_dir: Path) -> Path:\n",
    "    cands = sorted(models_dir.glob(\"best_*.pt\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    if not cands:\n",
    "        raise FileNotFoundError(\"No se encontró best_*.pt en models/local_checkpoints.\")\n",
    "    return cands[0]\n",
    "\n",
    "def try_get_production_model_version(client: MlflowClient, name: str):\n",
    "    try:\n",
    "        versions = client.search_model_versions(f\"name='{name}'\")\n",
    "        prod = [v for v in versions if getattr(v, \"current_stage\", \"\") == \"Production\"]\n",
    "        if prod:\n",
    "            # tomar el más nuevo\n",
    "            prod_sorted = sorted(prod, key=lambda x: int(x.version), reverse=True)\n",
    "            return prod_sorted[0]\n",
    "    except Exception:\n",
    "        return None\n",
    "    return None\n",
    "\n",
    "prod_mv = try_get_production_model_version(client, REGISTERED_MODEL_NAME)\n",
    "\n",
    "BASE_CKPT_PATH = None\n",
    "BASE_SOURCE = None\n",
    "\n",
    "if prod_mv is not None:\n",
    "  \n",
    "    BASE_SOURCE = f\"registry:/{REGISTERED_MODEL_NAME}/{prod_mv.version}\"\n",
    "    BASE_CKPT_PATH = find_latest_best_checkpoint(MODELS_DIR)\n",
    "else:\n",
    "    BASE_SOURCE = \"local:best_latest\"\n",
    "    BASE_CKPT_PATH = find_latest_best_checkpoint(MODELS_DIR)\n",
    "\n",
    "base_ckpt = torch.load(BASE_CKPT_PATH, map_location=\"cpu\")\n",
    "\n",
    "base_target_classes = base_ckpt[\"target_classes\"]\n",
    "NUM_CLASSES = len(base_target_classes) + 1\n",
    "\n",
    "model = build_model(NUM_CLASSES)\n",
    "model.load_state_dict(base_ckpt[\"model_state_dict\"])\n",
    "model.to(DEVICE)\n",
    "\n",
    "if not INCR_CONFIG[\"train_backbone\"]:\n",
    "    for p in model.backbone.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "for p in model.roi_heads.box_predictor.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "model.train()\n",
    "\n",
    "print(\"Base source:\", BASE_SOURCE)\n",
    "print(\"BASE_CKPT_PATH:\", BASE_CKPT_PATH.name)\n",
    "print(\"train_backbone:\", INCR_CONFIG[\"train_backbone\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d410b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "This cell:\n",
    "- Creates a small, fixed validation DataLoader (CPU-safe).\n",
    "- Computes val_loss_before using the base model (before incremental training).\n",
    "- Important: in torchvision detection, the loss is obtained with model.train() and no gradients.\n",
    "\"\"\"\n",
    "\n",
    "VAL_JSON = (PROCESSED_DIR / \"coco_person_car_airplane_val.json\").resolve()\n",
    "log(f\"VAL_JSON: {VAL_JSON}\")\n",
    "\n",
    "if not VAL_JSON.exists():\n",
    "    raise FileNotFoundError(\"No existe VAL_JSON reducido. Ejecuta Notebook 02.\")\n",
    "\n",
    "with open(VAL_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    val_coco = json.load(f)\n",
    "\n",
    "log(f\"Imágenes en JSON de validación: {len(val_coco.get('images', []))}\")\n",
    "log(f\"Anotaciones en JSON de validación: {len(val_coco.get('annotations', []))}\")\n",
    "\n",
    "name_to_id = labelmap[\"name_to_id\"]\n",
    "target_cat_ids_local = [int(name_to_id[n]) for n in TARGET_CLASSES]\n",
    "coco_to_internal_val = {cid: i + 1 for i, cid in enumerate(target_cat_ids_local)}\n",
    "\n",
    "log(f\"Clases objetivo (local ids): {coco_to_internal_val}\")\n",
    "\n",
    "class CocoValDataset(Dataset):\n",
    "    def __init__(self, images_dir: Path, coco_json: dict, max_images: int):\n",
    "        self.images_dir = images_dir\n",
    "        self.coco = coco_json\n",
    "        self.images = self.coco[\"images\"][:max_images]\n",
    "        self.annotations = self.coco[\"annotations\"]\n",
    "\n",
    "        self.img_id_to_anns = {}\n",
    "        for ann in self.annotations:\n",
    "            self.img_id_to_anns.setdefault(ann[\"image_id\"], []).append(ann)\n",
    "\n",
    "        self.id_to_image = {img[\"id\"]: img for img in self.images}\n",
    "        self.image_ids = list(self.id_to_image.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        img_id = self.image_ids[idx]\n",
    "        img_meta = self.id_to_image[img_id]\n",
    "        img_path = self.images_dir / img_meta[\"file_name\"]\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img_t = F.to_tensor(img)\n",
    "\n",
    "        anns = self.img_id_to_anns.get(img_id, [])\n",
    "        boxes, labels, areas, iscrowd = [], [], [], []\n",
    "\n",
    "        for a in anns:\n",
    "            cid = int(a[\"category_id\"])\n",
    "            if cid not in coco_to_internal_val:\n",
    "                continue\n",
    "            x, y, w, h = a[\"bbox\"]\n",
    "            boxes.append([x, y, x + w, y + h])\n",
    "            labels.append(coco_to_internal_val[cid])\n",
    "            areas.append(a.get(\"area\", w * h))\n",
    "            iscrowd.append(a.get(\"iscrowd\", 0))\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": torch.tensor(boxes, dtype=torch.float32),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.int64),\n",
    "            \"image_id\": torch.tensor([img_id], dtype=torch.int64),\n",
    "            \"area\": torch.tensor(areas, dtype=torch.float32),\n",
    "            \"iscrowd\": torch.tensor(iscrowd, dtype=torch.int64),\n",
    "        }\n",
    "        return img_t, target\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_loss_torchvision(model, data_loader) -> float:\n",
    "    log(\"Evaluando loss en modelo base (modo train sin grad)\")\n",
    "    was_training = model.training\n",
    "    model.train()\n",
    "\n",
    "    total = 0.0\n",
    "    n = 0\n",
    "\n",
    "    for images, targets in tqdm(data_loader, desc=\"val_loss_check\", leave=False):\n",
    "        images = [img.to(DEVICE) for img in images]\n",
    "        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        total += float(losses.item())\n",
    "        n += 1\n",
    "\n",
    "    if not was_training:\n",
    "        model.eval()\n",
    "\n",
    "    avg_loss = total / max(1, n)\n",
    "    log(f\"val_loss promedio calculado: {avg_loss:.6f}\")\n",
    "    return avg_loss\n",
    "\n",
    "log(\"Creando DataLoader de validación (CPU-safe) ...\")\n",
    "eval_ds = CocoValDataset(\n",
    "    VAL_IMG_DIR,\n",
    "    val_coco,\n",
    "    max_images=INCR_CONFIG[\"eval_max_images\"]\n",
    ")\n",
    "log(f\"Imágenes usadas para evaluación: {len(eval_ds)}\")\n",
    "\n",
    "eval_loader = DataLoader(\n",
    "    eval_ds,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "log(\"Calculando val_loss_before ...\")\n",
    "val_loss_before = evaluate_loss_torchvision(model, eval_loader)\n",
    "\n",
    "log(f\"val_loss_check_before_incremental: {val_loss_before:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e39e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 | train_loss_newdata=0.4238 | time=28.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 | train_loss_newdata=0.3497 | time=24.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registered model: frcnn_coco_cpu_person_car_airplane version: 14\n",
      "Promoted to Production: 14\n",
      "Manifest actualizado. Nuevas marcadas como usadas: 7\n",
      "Incremental terminado\n",
      "best_incr_ckpt_path: c:\\Users\\Johnny\\Desktop\\IA-final\\models\\local_checkpoints\\best_incr_incr_train_20260204_130559.pt\n",
      "last_epoch_ckpt_path: c:\\Users\\Johnny\\Desktop\\IA-final\\models\\local_checkpoints\\epoch_2_incr_incr_train_20260204_130559.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'frcnn_coco_cpu_person_car_airplane' already exists. Creating a new version of this model...\n",
      "Created version '14' of model 'frcnn_coco_cpu_person_car_airplane'.\n",
      "C:\\Users\\Johnny\\AppData\\Local\\Temp\\ipykernel_31740\\3873823160.py:197: FutureWarning: ``mlflow.tracking.client.MlflowClient.transition_model_version_stage`` is deprecated since 2.9.0. Model registry stages will be removed in a future major release. To learn more about the deprecation of model registry stages, see our migration guide here: https://mlflow.org/docs/latest/model-registry.html#migrating-from-stages\n",
      "  client.transition_model_version_stage(\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This cell:\n",
    "- Trains ONLY with incr_loader (new images).\n",
    "- Saves a checkpoint every epoch (always) plus a best model (based on train_loss_newdata).\n",
    "- Logs everything inside an incremental MLflow run.\n",
    "\"\"\"\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# =========================\n",
    "# LOG EVENT (JSONL)\n",
    "# =========================\n",
    "RETRAIN_LOG = (LOGS_DIR / \"retrain_progress.log\").resolve()\n",
    "\n",
    "def log_event(event: dict):\n",
    "    \"\"\"\n",
    "    Escribe eventos en formato JSONL (1 evento por línea).\n",
    "    El frontend lo parsea para barra/epoch/loss/eta.\n",
    "    \"\"\"\n",
    "    event[\"ts\"] = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    with open(RETRAIN_LOG, \"a\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        f.write(json.dumps(event, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# OPTIMIZER\n",
    "# =========================\n",
    "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    trainable_params,\n",
    "    lr=INCR_CONFIG[\"learning_rate\"],\n",
    "    weight_decay=INCR_CONFIG[\"weight_decay\"]\n",
    ")\n",
    "\n",
    "def train_one_epoch_incremental(model, data_loader, optimizer, epoch: int) -> float:\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    n = 0\n",
    "\n",
    "    for images, targets in tqdm(data_loader, desc=f\"incr train e{epoch}\", leave=False):\n",
    "        images = [img.to(DEVICE) for img in images]\n",
    "        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += float(losses.item())\n",
    "        n += 1\n",
    "\n",
    "    return total_loss / max(1, n)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# RUN SETUP\n",
    "# =========================\n",
    "incr_run_name = f\"incr_train_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "best_train_loss = float(\"inf\")\n",
    "best_incr_ckpt_path = None\n",
    "last_epoch_ckpt_path = None\n",
    "\n",
    "# Guardar lista de archivos nuevos usados (solo NUEVOS, para que tenga sentido)\n",
    "used_list_path = MODELS_DIR / f\"new_files_{incr_run_name}.json\"\n",
    "with open(used_list_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump([{\"image\": p[0].name, \"label\": p[1].name} for p in pairs_new], f, indent=2, ensure_ascii=False)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# MLFLOW RUN\n",
    "# =========================\n",
    "with mlflow.start_run(run_name=incr_run_name) as run:\n",
    "    incr_run_id = run.info.run_id\n",
    "\n",
    "    mlflow.set_tag(\"stage\", \"incremental\")\n",
    "    mlflow.set_tag(\"status\", \"TRAINED\")\n",
    "    mlflow.set_tag(\"parent_checkpoint\", BASE_CKPT_PATH.name)\n",
    "    mlflow.set_tag(\"parent_source\", BASE_SOURCE)\n",
    "    mlflow.set_tag(\"classes\", \",\".join(TARGET_CLASSES))\n",
    "    mlflow.set_tag(\"registered_model_name\", REGISTERED_MODEL_NAME)\n",
    "\n",
    "    mlflow.log_params(INCR_CONFIG)\n",
    "    mlflow.log_param(\"new_images\", len(pairs_new))          \n",
    "    mlflow.log_param(\"train_images_total\", len(pairs_train)) \n",
    "\n",
    "    mlflow.log_metric(\"val_loss_before\", val_loss_before)\n",
    "\n",
    "    mlflow.log_artifact(str(used_list_path), artifact_path=\"artifacts\")\n",
    "    mlflow.log_artifact(str(PROJECT_CONFIG_PATH), artifact_path=\"artifacts\")\n",
    "    mlflow.log_artifact(str(LABELMAP_PATH), artifact_path=\"artifacts\")\n",
    "\n",
    "    # =========================\n",
    "    # EVENTO START\n",
    "    # =========================\n",
    "    log_event({\n",
    "        \"type\": \"start\",\n",
    "        \"epochs_total\": INCR_CONFIG[\"epochs\"],\n",
    "        \"run_name\": incr_run_name,\n",
    "        \"run_id\": incr_run_id,\n",
    "        \"new_images\": int(len(pairs_new)),            \n",
    "        \"train_images_total\": int(len(pairs_train)),  \n",
    "        \"val_loss_before\": float(val_loss_before) if val_loss_before is not None else None,\n",
    "    })\n",
    "\n",
    "    # =========================\n",
    "    # TRAIN LOOP\n",
    "    # =========================\n",
    "    for epoch in range(1, INCR_CONFIG[\"epochs\"] + 1):\n",
    "        t0 = time.time()\n",
    "\n",
    "        train_loss = train_one_epoch_incremental(model, incr_loader, optimizer, epoch)\n",
    "        epoch_time = time.time() - t0\n",
    "\n",
    "        mlflow.log_metric(\"train_loss_newdata\", train_loss, step=epoch)\n",
    "        mlflow.log_metric(\"epoch_time_sec\", epoch_time, step=epoch)\n",
    "\n",
    "        # checkpoint por época (SIEMPRE)\n",
    "        epoch_ckpt_path = MODELS_DIR / f\"epoch_{epoch}_incr_{incr_run_name}.pt\"\n",
    "        last_epoch_ckpt_path = epoch_ckpt_path\n",
    "\n",
    "        ckpt_out = {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"parent_checkpoint\": str(BASE_CKPT_PATH),\n",
    "            \"parent_source\": BASE_SOURCE,\n",
    "            \"incr_config\": INCR_CONFIG,\n",
    "            \"target_classes\": TARGET_CLASSES,\n",
    "        }\n",
    "\n",
    "        torch.save(ckpt_out, epoch_ckpt_path)\n",
    "        mlflow.log_artifact(str(epoch_ckpt_path), artifact_path=\"checkpoints\")\n",
    "\n",
    "        print(f\"Epoch {epoch}/{INCR_CONFIG['epochs']} | train_loss_newdata={train_loss:.4f} | time={epoch_time:.1f}s\")\n",
    "\n",
    "        if train_loss < best_train_loss:\n",
    "            best_train_loss = train_loss\n",
    "            best_incr_ckpt_path = MODELS_DIR / f\"best_incr_{incr_run_name}.pt\"\n",
    "            torch.save({**ckpt_out, \"best_train_loss_newdata\": best_train_loss}, best_incr_ckpt_path)\n",
    "            mlflow.log_artifact(str(best_incr_ckpt_path), artifact_path=\"checkpoints\")\n",
    "\n",
    "        # =========================\n",
    "        # EVENTO EPOCH\n",
    "        # =========================\n",
    "        log_event({\n",
    "            \"type\": \"epoch\",\n",
    "            \"epoch\": int(epoch),\n",
    "            \"epochs_total\": int(INCR_CONFIG[\"epochs\"]),\n",
    "            \"train_loss\": float(train_loss),\n",
    "            \"val_loss\": None,  # no evaluamos por epoch en este notebook\n",
    "            \"epoch_time_sec\": float(epoch_time),\n",
    "        })\n",
    "\n",
    "    # =========================\n",
    "    # POST-TRAIN METRICS\n",
    "    # =========================\n",
    "    mlflow.log_metric(\"best_train_loss_newdata\", best_train_loss)\n",
    "\n",
    "    # Eval AFTER\n",
    "    val_loss_after = evaluate_loss_torchvision(model, eval_loader)\n",
    "    mlflow.log_metric(\"val_loss_after\", val_loss_after)\n",
    "\n",
    "    # =========================\n",
    "    # MODEL REGISTRY\n",
    "    # =========================\n",
    "    from mlflow.tracking import MlflowClient\n",
    "    client = MlflowClient()\n",
    "\n",
    "    MODEL_NAME = REGISTERED_MODEL_NAME\n",
    "    run_id = mlflow.active_run().info.run_id\n",
    "\n",
    "    if best_incr_ckpt_path is None:\n",
    "        best_incr_ckpt_path = last_epoch_ckpt_path\n",
    "\n",
    "    model_source = f\"runs:/{run_id}/checkpoints/{best_incr_ckpt_path.name}\"\n",
    "\n",
    "    registered = False\n",
    "    promoted_version = None\n",
    "\n",
    "    if True:\n",
    "        mv = mlflow.register_model(model_source, MODEL_NAME)\n",
    "        registered = True\n",
    "        promoted_version = mv.version\n",
    "        print(\"Registered model:\", MODEL_NAME, \"version:\", mv.version)\n",
    "\n",
    "        try:\n",
    "            client.transition_model_version_stage(\n",
    "                name=MODEL_NAME,\n",
    "                version=mv.version,\n",
    "                stage=\"Production\",\n",
    "                archive_existing_versions=True\n",
    "            )\n",
    "            print(\"Promoted to Production:\", mv.version)\n",
    "        except Exception as e:\n",
    "            print(\"Stage transition skipped:\", str(e))\n",
    "    else:\n",
    "        print(\"No se registró nueva versión porque no mejoró.\")\n",
    "\n",
    "  \n",
    "    log_event({\n",
    "        \"type\": \"done\",\n",
    "        \"status\": \"OK\",\n",
    "        \"best_train_loss\": float(best_train_loss),\n",
    "        \"val_loss_after\": float(val_loss_after),\n",
    "        \"registered\": bool(registered),\n",
    "        \"production_version\": int(promoted_version) if promoted_version is not None else None,\n",
    "    })\n",
    "\n",
    "\n",
    "    manifest = load_manifest(MANIFEST_PATH)\n",
    "    used_files = set(manifest.get(\"used_files\", []))\n",
    "\n",
    "    for img_path, _ in pairs_new:\n",
    "        used_files.add(img_path.name)\n",
    "\n",
    "    manifest[\"used_files\"] = sorted(list(used_files))\n",
    "    save_manifest(MANIFEST_PATH, manifest)\n",
    "\n",
    "    print(\"Manifest actualizado. Nuevas marcadas como usadas:\", len(pairs_new))\n",
    "\n",
    "\n",
    "print(\"Incremental terminado\")\n",
    "print(\"best_incr_ckpt_path:\", best_incr_ckpt_path)\n",
    "print(\"last_epoch_ckpt_path:\", last_epoch_ckpt_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca18ce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_loss_check_after_incremental: 0.6454900339245796\n",
      "Improved: True\n",
      "Delta used: 0.0\n",
      "Before: 0.6709062796086073 After: 0.6454900339245796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This cell:\n",
    "- Computes val_loss_after using the same eval_loader.\n",
    "- Determines whether the model improved by comparing it against val_loss_before.\n",
    "\"\"\"\n",
    "\n",
    "val_loss_after = evaluate_loss_torchvision(model, eval_loader)\n",
    "print(\"val_loss_check_after_incremental:\", val_loss_after)\n",
    "\n",
    "improvement_delta = float(INCR_CONFIG[\"improvement_delta\"])\n",
    "improved = (val_loss_after + improvement_delta) < val_loss_before\n",
    "\n",
    "print(\"Improved:\", improved)\n",
    "print(\"Delta used:\", improvement_delta)\n",
    "print(\"Before:\", val_loss_before, \"After:\", val_loss_after)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f28ce84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivos marcados como usados y archivados en: C:\\Users\\Johnny\\Desktop\\IA-final\\data\\new_data\\used\\incr_train_20260204_130559\n",
      "Manifest actualizado: C:\\Users\\Johnny\\Desktop\\IA-final\\data\\new_data\\manifest.json\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This cell:\n",
    "- Updates manifest.json to prevent retraining twice on the same images.\n",
    "- Moves images and labels to new_data/used/<run_name>/ for archiving.\n",
    "\"\"\"\n",
    "\n",
    "archive_dir = (NEW_USED_DIR / incr_run_name).resolve()\n",
    "archive_img = (archive_dir / \"images\").resolve()\n",
    "archive_lbl = (archive_dir / \"labels\").resolve()\n",
    "archive_img.mkdir(parents=True, exist_ok=True)\n",
    "archive_lbl.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for img_path, lbl_path in new_pairs:\n",
    "    used.add(img_path.name)\n",
    "    shutil.move(str(img_path), str(archive_img / img_path.name))\n",
    "    shutil.move(str(lbl_path), str(archive_lbl / lbl_path.name))\n",
    "\n",
    "manifest[\"used_files\"] = sorted(list(used))\n",
    "save_manifest(MANIFEST_PATH, manifest)\n",
    "\n",
    "print(\"Archivos marcados como usados y archivados en:\", archive_dir)\n",
    "print(\"Manifest actualizado:\", MANIFEST_PATH)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
