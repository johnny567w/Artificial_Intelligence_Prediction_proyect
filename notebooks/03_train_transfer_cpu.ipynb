{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8035d69a",
   "metadata": {},
   "source": [
    "# Third Notebook\n",
    "\n",
    "In this notebook, the JSON files created from the reduced dataset will be integrated for training. A pretrained Faster R-CNN model will be used due to its accuracy and resource efficiency. Additionally, model tracking in MLflow will be initiated.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "1. Load the reduced dataset generated in `data/processed/`.\n",
    "2. Create a DataLoader compatible with detection models (torchvision).\n",
    "3. Build a pretrained model (Faster R-CNN) and adapt the head to 3 classes.\n",
    "4. Train on CPU with an efficient configuration.\n",
    "5. Log to MLflow:\n",
    "   - parameters\n",
    "   - metrics\n",
    "   - artifacts (weights, labelmap, config)\n",
    "6. Save a local checkpoint for inference and for incremental retraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49098dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- Imports the necessary libraries for training.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import mlflow\n",
    "\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.transforms import functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d270675",
   "metadata": {},
   "source": [
    "1. Loading .json files and validating the project structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bed0371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: C:\\Users\\Johnny\\Desktop\\IA-final\n",
      "TRAIN_JSON: C:\\Users\\Johnny\\Desktop\\IA-final\\data\\processed\\coco_person_car_airplane_train.json\n",
      "VAL_JSON  : C:\\Users\\Johnny\\Desktop\\IA-final\\data\\processed\\coco_person_car_airplane_val.json\n",
      "TARGET_CLASSES: ['person', 'car', 'airplane']\n",
      "target_cat_ids: [1, 3, 5]\n",
      "MODELS_DIR: C:\\Users\\Johnny\\Desktop\\IA-final\\models\\local_checkpoints\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "In this section, project_config.json and labelmap.json created in Notebook 01 are loaded.\n",
    "It also defines absolute paths without depending on the notebook’s current working directory.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def find_project_root(start: Path, max_up: int = 8) -> Path:\n",
    "    cur = start.resolve()\n",
    "    for _ in range(max_up):\n",
    "        if (cur / \"data\" / \"processed\" / \"project_config.json\").exists():\n",
    "            return cur\n",
    "        cur = cur.parent\n",
    "    raise FileNotFoundError(\"No se encontró data/processed/project_config.json. Ejecuta Notebook 01.\")\n",
    "\n",
    "PROJECT_ROOT = find_project_root(Path.cwd())\n",
    "PROCESSED_DIR = (PROJECT_ROOT / \"data\" / \"processed\").resolve()\n",
    "\n",
    "PROJECT_CONFIG_PATH = (PROCESSED_DIR / \"project_config.json\").resolve()\n",
    "LABELMAP_PATH = (PROCESSED_DIR / \"labelmap.json\").resolve()\n",
    "\n",
    "with open(PROJECT_CONFIG_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    project_config = json.load(f)\n",
    "\n",
    "with open(LABELMAP_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    labelmap = json.load(f)\n",
    "\n",
    "COCO_ROOT = Path(project_config[\"coco_root\"])\n",
    "TRAIN_IMG_DIR = Path(project_config[\"train_dir\"])\n",
    "VAL_IMG_DIR = Path(project_config[\"val_dir\"])\n",
    "\n",
    "TARGET_CLASSES = project_config[\"target_classes\"]\n",
    "target_cat_ids = labelmap[\"target_category_ids\"]\n",
    "\n",
    "TRAIN_JSON = (PROCESSED_DIR / \"coco_person_car_airplane_train.json\").resolve()\n",
    "VAL_JSON = (PROCESSED_DIR / \"coco_person_car_airplane_val.json\").resolve()\n",
    "\n",
    "MODELS_DIR = (PROJECT_ROOT / \"models\" / \"local_checkpoints\").resolve()\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Mismo contenido visto anteriormente, solo para confirmar las carpetas \")\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"TRAIN_JSON:\", TRAIN_JSON)\n",
    "print(\"VAL_JSON  :\", VAL_JSON)\n",
    "print(\"TARGET_CLASSES:\", TARGET_CLASSES)\n",
    "print(\"target_cat_ids:\", target_cat_ids)\n",
    "print(\"MODELS_DIR:\", MODELS_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa133bb",
   "metadata": {},
   "source": [
    "2. Limited training configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9016a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFIG:\n",
      "seed: 42\n",
      "device: cpu\n",
      "cpu_num_threads: 11\n",
      "batch_size: 2\n",
      "num_workers: 0\n",
      "learning_rate: 0.0001\n",
      "weight_decay: 0.0001\n",
      "epochs: 2\n",
      "score_threshold: 0.5\n",
      "model_name: fasterrcnn_resnet50_fpn\n",
      "pretrained_weights: DEFAULT\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "In this section, CPU-based training is configured, since a powerful GPU is not available on the machine used for training.\n",
    "If you have access to a high-performance GPU, you can adjust these parameters to speed up the training process.\n",
    "Additionally, threads and random seeds are set to ensure reproducibility and prevent random image selection.\n",
    "Conservative hyperparameters suitable for CPU training are also defined.\n",
    "\"\"\"\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "CPU_NUM_THREADS = max(1, (os.cpu_count() or 2) - 1)\n",
    "torch.set_num_threads(CPU_NUM_THREADS)\n",
    "\n",
    "CONFIG = {\n",
    "    \"seed\": SEED,\n",
    "    \"device\": str(DEVICE),\n",
    "    \"cpu_num_threads\": CPU_NUM_THREADS,\n",
    "    \"batch_size\": 2,\n",
    "    \"num_workers\": 0,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"weight_decay\": 1e-4,\n",
    "    \"epochs\": 2,\n",
    "    \"score_threshold\": 0.5,\n",
    "    \"model_name\": \"fasterrcnn_resnet50_fpn\",\n",
    "    \"pretrained_weights\": \"DEFAULT\",\n",
    "}\n",
    "\n",
    "print(\"CONFIG:\")\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"{k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4325b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN images: 3300\n",
      "VAL images  : 500\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Here, the main goal is to verify that TRAIN_JSON and VAL_JSON exist.\n",
    "It also uses the previously defined limits on the number of images they contain.\n",
    "If any abnormal situation occurs, execution is halted if the training set exceeds the established limit.\n",
    "\"\"\"\n",
    "\n",
    "def assert_file(path: Path, label: str) -> None:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Archivo requerido no encontrado: {label}: {path}\")\n",
    "\n",
    "assert_file(TRAIN_JSON, \"TRAIN_JSON reducido (Notebook 02)\")\n",
    "assert_file(VAL_JSON, \"VAL_JSON reducido (Notebook 02)\")\n",
    "\n",
    "with open(TRAIN_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    train_coco = json.load(f)\n",
    "\n",
    "with open(VAL_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    val_coco = json.load(f)\n",
    "\n",
    "train_images_n = len(train_coco[\"images\"])\n",
    "val_images_n = len(val_coco[\"images\"])\n",
    "\n",
    "print(\"TRAIN images:\", train_images_n)\n",
    "print(\"VAL images  :\", val_images_n)\n",
    "\n",
    "MAX_ALLOWED_TRAIN_IMAGES = 10000  # seguridad dura\n",
    "if train_images_n > MAX_ALLOWED_TRAIN_IMAGES:\n",
    "    raise RuntimeError(\n",
    "        f\"TRAIN_JSON tiene {train_images_n} imágenes, supera el límite {MAX_ALLOWED_TRAIN_IMAGES}. \"\n",
    "        \"Regenera el JSON en Notebook 02 con recorte.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f5550b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In this section, a custom Dataset is defined using the dataset previously reduced in the second notebook, designed to work with COCO-format data and specifically adapted for torchvision object detection models.\n",
    "The main objective is to load the images, process their annotations, and return the data in the exact format required by the model for training.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class CocoReducedDetectionDataset(Dataset):\n",
    "    def __init__(self, images_dir: Path, coco_json: dict):\n",
    "        self.images_dir = images_dir\n",
    "        self.coco = coco_json\n",
    "\n",
    "        self.images = self.coco[\"images\"]\n",
    "        self.annotations = self.coco[\"annotations\"]\n",
    "\n",
    "        self.img_id_to_anns = {}\n",
    "        for ann in self.annotations:\n",
    "            self.img_id_to_anns.setdefault(ann[\"image_id\"], []).append(ann)\n",
    "\n",
    "        self.id_to_image = {img[\"id\"]: img for img in self.images}\n",
    "        self.image_ids = list(self.id_to_image.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        img_id = self.image_ids[idx]\n",
    "        img_meta = self.id_to_image[img_id]\n",
    "        img_path = self.images_dir / img_meta[\"file_name\"]\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img_tensor = F.to_tensor(img)\n",
    "\n",
    "        anns = self.img_id_to_anns.get(img_id, [])\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        areas = []\n",
    "        iscrowd = []\n",
    "\n",
    "        for a in anns:\n",
    "            x, y, w, h = a[\"bbox\"]\n",
    "            boxes.append([x, y, x + w, y + h])\n",
    "            labels.append(a[\"category_id\"])\n",
    "            areas.append(a.get(\"area\", w * h))\n",
    "            iscrowd.append(a.get(\"iscrowd\", 0))\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": torch.tensor(boxes, dtype=torch.float32),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.int64),\n",
    "            \"image_id\": torch.tensor([img_id], dtype=torch.int64),\n",
    "            \"area\": torch.tensor(areas, dtype=torch.float32),\n",
    "            \"iscrowd\": torch.tensor(iscrowd, dtype=torch.int64),\n",
    "        }\n",
    "\n",
    "        return img_tensor, target\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, targets = zip(*batch)\n",
    "    return list(images), list(targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85975dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 3300\n",
      "Val samples  : 500\n",
      "Train iters per epoch: 1650\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "In this section, the training and validation datasets are created, and then the DataLoaders that will be used during model training are built.\n",
    "It also ensures that train_loader and val_loader exist before starting the training process.\n",
    "\"\"\"\n",
    "\n",
    "train_ds = CocoReducedDetectionDataset(TRAIN_IMG_DIR, train_coco)\n",
    "val_ds = CocoReducedDetectionDataset(VAL_IMG_DIR, val_coco)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=CONFIG[\"batch_size\"],\n",
    "    shuffle=True,\n",
    "    num_workers=CONFIG[\"num_workers\"],\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=CONFIG[\"num_workers\"],\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "print(\"Train samples:\", len(train_ds))\n",
    "print(\"Val samples  :\", len(val_ds))\n",
    "print(\"Train iters per epoch:\", len(train_loader))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cc2f4c",
   "metadata": {},
   "source": [
    "2. Model construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269ca459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM_CLASSES: 4\n",
      "coco_to_internal: {1: 1, 3: 2, 5: 3}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "In this section, the Faster R-CNN detection model is built using pretrained weights,\n",
    "and its architecture is adjusted to work exclusively with the project’s target classes.\n",
    "Since COCO category identifiers are not consecutive and do not start from 1,\n",
    "the original category_id values are remapped to internal labels ranging from 1..K,\n",
    "with 0 reserved for the background class, as required by the model.\n",
    "Additionally, the classification head of the model is redefined so that the number of outputs matches the total number of target classes plus the background class.\n",
    "\"\"\"\n",
    "\n",
    "target_cat_ids_int = [int(x) for x in target_cat_ids]\n",
    "coco_to_internal = {cid: i + 1 for i, cid in enumerate(target_cat_ids_int)}\n",
    "internal_to_name = {i + 1: name for i, name in enumerate(TARGET_CLASSES)}\n",
    "\n",
    "NUM_CLASSES = len(TARGET_CLASSES) + 1\n",
    "\n",
    "def remap_targets(targets: List[Dict]) -> List[Dict]:\n",
    "    out = []\n",
    "    for t in targets:\n",
    "        new_t = t.copy()\n",
    "        labels = t[\"labels\"].clone()\n",
    "        for i in range(labels.shape[0]):\n",
    "            labels[i] = coco_to_internal[int(labels[i].item())]\n",
    "        new_t[\"labels\"] = labels\n",
    "        out.append(new_t)\n",
    "    return out\n",
    "\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=CONFIG[\"pretrained_weights\"])\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, NUM_CLASSES)\n",
    "\n",
    "model.to(DEVICE)\n",
    "model.train()\n",
    "\n",
    "print(\"NUM_CLASSES:\", NUM_CLASSES)\n",
    "print(\"coco_to_internal:\", coco_to_internal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b774294",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In this section, the AdamW optimizer is defined to update the model’s weights during training, as it provides good stability and helps control overfitting through weight_decay.\n",
    "Additionally, the epoch-level training (train_one_epoch) and evaluation (evaluate_loss) functions are implemented, where the average loss per epoch is computed.\n",
    "This is particularly useful for monitoring CPU-based training and for comparing performance between training and validation phases.\n",
    "\"\"\"\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=CONFIG[\"learning_rate\"],\n",
    "    weight_decay=CONFIG[\"weight_decay\"]\n",
    ")\n",
    "\n",
    "def train_one_epoch(model, data_loader, optimizer, epoch: int) -> float:\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    n = 0\n",
    "\n",
    "    for images, targets in tqdm(data_loader, desc=f\"train e{epoch}\", leave=False):\n",
    "        images = [img.to(DEVICE) for img in images]\n",
    "        targets = remap_targets(targets)\n",
    "        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += float(losses.item())\n",
    "        n += 1\n",
    "\n",
    "    return total_loss / max(1, n)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_loss(model, data_loader) -> float:\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    n = 0\n",
    "\n",
    "    for images, targets in tqdm(data_loader, desc=\"val\", leave=False):\n",
    "        images = [img.to(DEVICE) for img in images]\n",
    "        targets = remap_targets(targets)\n",
    "        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        total_loss += float(losses.item())\n",
    "        n += 1\n",
    "\n",
    "    model.train()\n",
    "    return total_loss / max(1, n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4217cc31",
   "metadata": {},
   "source": [
    "3. Uso del entrenamiento con dataset reducido, modelo preentrenado y optimizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007961d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/01 08:34:46 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.schemas\n",
      "2026/02/01 08:34:46 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.tables\n",
      "2026/02/01 08:34:46 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.types\n",
      "2026/02/01 08:34:46 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.constraints\n",
      "2026/02/01 08:34:46 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.defaults\n",
      "2026/02/01 08:34:46 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.comments\n",
      "2026/02/01 08:34:48 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n",
      "2026/02/01 08:34:48 INFO mlflow.store.db.utils: Updating database tables\n",
      "2026/02/01 08:34:48 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n",
      "2026/02/01 08:34:48 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n",
      "2026/02/01 08:34:48 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n",
      "2026/02/01 08:34:48 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n",
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 | train_loss=0.4747 | val_loss=0.4836 | time=21692.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 | train_loss=0.4070 | val_loss=0.4406 | time=20395.6s\n",
      "Entrenamiento finalizado\n",
      "best_val_loss: 0.44062818501889706\n",
      "best_ckpt_path: C:\\Users\\Johnny\\Desktop\\IA-final\\models\\local_checkpoints\\best_frcnn_cpu_base_train_20260201_083448.pt\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This cell:\n",
    "- Configures MLflow with SQLite (mlflow.db) to avoid FileStore deprecation issues.\n",
    "- Logs parameters, metrics, and artifacts (mappings + con\n",
    "\"\"\"\n",
    "# -----------------------------\n",
    "# MLflow: SQLite backend\n",
    "# -----------------------------\n",
    "MLFLOW_DB = (PROJECT_ROOT / \"mlflow.db\").resolve()\n",
    "mlflow.set_tracking_uri(f\"sqlite:///{MLFLOW_DB.as_posix()}\")\n",
    "\n",
    "EXPERIMENT_NAME = \"object_detection_coco_cpu\"\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "run_name = f\"base_train_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "best_ckpt_path = None\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_loss(model, data_loader) -> float:\n",
    "    \"\"\"\n",
    "    Esta función:\n",
    "    - Calcula pérdida en validación sin gradientes.\n",
    "    - En modelos de detección torchvision, el diccionario de pérdidas se obtiene en modo train.\n",
    "    - Por eso forzamos model.train() temporalmente y luego restauramos el estado previo.\n",
    "    \"\"\"\n",
    "    was_training = model.training\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    n = 0\n",
    "\n",
    "    for images, targets in tqdm(data_loader, desc=\"val\", leave=False):\n",
    "        images = [img.to(DEVICE) for img in images]\n",
    "        targets = remap_targets(targets)\n",
    "        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)  # dict de pérdidas\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        total_loss += float(losses.item())\n",
    "        n += 1\n",
    "\n",
    "    if not was_training:\n",
    "        model.eval()\n",
    "\n",
    "    return total_loss / max(1, n)\n",
    "\n",
    "\n",
    "with mlflow.start_run(run_name=run_name):\n",
    "    # -----------------------------\n",
    "    # Params + tags\n",
    "    # -----------------------------\n",
    "    mlflow.log_params(CONFIG)\n",
    "    mlflow.set_tag(\"stage\", \"base\")\n",
    "    mlflow.set_tag(\"train_type\", \"cpu\")\n",
    "    mlflow.set_tag(\"classes\", \",\".join(TARGET_CLASSES))\n",
    "    mlflow.set_tag(\"model_arch\", CONFIG[\"model_name\"])\n",
    "\n",
    "    # -----------------------------\n",
    "    # Artefactos de configuración/mapping\n",
    "    # -----------------------------\n",
    "    mapping_artifact = {\n",
    "        \"target_classes\": TARGET_CLASSES,\n",
    "        \"target_category_ids\": target_cat_ids_int,\n",
    "        \"coco_to_internal\": coco_to_internal,\n",
    "        \"internal_to_name\": internal_to_name,\n",
    "        \"train_json\": str(TRAIN_JSON.as_posix()),\n",
    "        \"val_json\": str(VAL_JSON.as_posix()),\n",
    "    }\n",
    "\n",
    "    mapping_path = MODELS_DIR / f\"mapping_{run_name}.json\"\n",
    "    with open(mapping_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(mapping_artifact, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    mlflow.log_artifact(str(mapping_path), artifact_path=\"artifacts\")\n",
    "    mlflow.log_artifact(str(PROJECT_CONFIG_PATH), artifact_path=\"artifacts\")\n",
    "    mlflow.log_artifact(str(LABELMAP_PATH), artifact_path=\"artifacts\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # Entrenamiento por épocas\n",
    "    # -----------------------------\n",
    "    for epoch in range(1, CONFIG[\"epochs\"] + 1):\n",
    "        t0 = time.time()\n",
    "\n",
    "        # TRAIN\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, epoch)\n",
    "\n",
    "        # CHECKPOINT por época (SIEMPRE, antes de validar)\n",
    "        epoch_ckpt_path = MODELS_DIR / f\"epoch_{epoch}_frcnn_cpu_{run_name}.pt\"\n",
    "        epoch_ckpt = {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"config\": CONFIG,\n",
    "            \"target_classes\": TARGET_CLASSES,\n",
    "            \"target_category_ids\": target_cat_ids_int,\n",
    "            \"coco_to_internal\": coco_to_internal,\n",
    "            \"internal_to_name\": internal_to_name,\n",
    "        }\n",
    "        torch.save(epoch_ckpt, epoch_ckpt_path)\n",
    "        mlflow.log_artifact(str(epoch_ckpt_path), artifact_path=\"checkpoints\")\n",
    "\n",
    "        # VALIDATION (loss)\n",
    "        val_loss = evaluate_loss(model, val_loader)\n",
    "\n",
    "        epoch_time = time.time() - t0\n",
    "\n",
    "        # LOG metrics\n",
    "        mlflow.log_metric(\"train_loss\", train_loss, step=epoch)\n",
    "        mlflow.log_metric(\"val_loss\", val_loss, step=epoch)\n",
    "        mlflow.log_metric(\"epoch_time_sec\", epoch_time, step=epoch)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch}/{CONFIG['epochs']} | \"\n",
    "            f\"train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | time={epoch_time:.1f}s\"\n",
    "        )\n",
    "\n",
    "        # BEST model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_ckpt_path = MODELS_DIR / f\"best_frcnn_cpu_{run_name}.pt\"\n",
    "\n",
    "            best_ckpt = epoch_ckpt.copy()\n",
    "            best_ckpt[\"best_val_loss\"] = best_val_loss\n",
    "\n",
    "            torch.save(best_ckpt, best_ckpt_path)\n",
    "            mlflow.log_artifact(str(best_ckpt_path), artifact_path=\"checkpoints\")\n",
    "\n",
    "    mlflow.log_metric(\"best_val_loss\", best_val_loss)\n",
    "\n",
    "print(\"Entrenamiento finalizado\")\n",
    "print(\"best_val_loss:\", best_val_loss)\n",
    "print(\"best_ckpt_path:\", best_ckpt_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf35bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to C:\\Users\\Johnny/.cache\\torch\\hub\\checkpoints\\resnet50-0676ba61.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97.8M/97.8M [00:03<00:00, 31.1MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagen: C:\\Users\\Johnny\\Desktop\\IA-final\\data\\archive\\coco2017\\val2017\\000000397133.jpg\n",
      "{'name': 'person', 'score': 0.9935327768325806, 'box_xyxy': [385.1822204589844, 76.41061401367188, 497.1370544433594, 343.8106994628906]}\n",
      "{'name': 'person', 'score': 0.9191341400146484, 'box_xyxy': [352.98193359375, 97.65465545654297, 404.5033264160156, 238.57272338867188]}\n",
      "{'name': 'person', 'score': 0.6582873463630676, 'box_xyxy': [199.18922424316406, 9.169189453125, 283.14971923828125, 101.2300796508789]}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This section is mainly used to ensure model reliability for\n",
    "\"\"\"\n",
    "\n",
    "@torch.no_grad()\n",
    "def load_model_from_ckpt(ckpt_path: Path):\n",
    "    ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=None)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    num_classes = len(ckpt[\"target_classes\"]) + 1\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "    model.eval()\n",
    "    return model, ckpt\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_top3(model, image_path: Path, internal_to_name: Dict[int, str], score_thr: float = 0.5):\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    img_t = F.to_tensor(img)\n",
    "\n",
    "    outputs = model([img_t])[0]\n",
    "    boxes = outputs[\"boxes\"].cpu()\n",
    "    scores = outputs[\"scores\"].cpu()\n",
    "    labels = outputs[\"labels\"].cpu()\n",
    "\n",
    "    keep = scores >= score_thr\n",
    "    boxes = boxes[keep]\n",
    "    scores = scores[keep]\n",
    "    labels = labels[keep]\n",
    "\n",
    "    if len(scores) == 0:\n",
    "        return []\n",
    "\n",
    "    topk = min(3, len(scores))\n",
    "    idx = torch.argsort(scores, descending=True)[:topk]\n",
    "\n",
    "    results = []\n",
    "    for i in idx:\n",
    "        lbl = int(labels[i].item())\n",
    "        results.append({\n",
    "            \"name\": internal_to_name.get(lbl, \"unknown\"),\n",
    "            \"score\": float(scores[i].item()),\n",
    "            \"box_xyxy\": boxes[i].tolist(),\n",
    "        })\n",
    "    return results\n",
    "\n",
    "if best_ckpt_path is None:\n",
    "    raise FileNotFoundError(\"No hay best_ckpt_path. Revisa el entrenamiento.\")\n",
    "\n",
    "infer_model, ckpt = load_model_from_ckpt(best_ckpt_path)\n",
    "\n",
    "sample_img_meta = val_coco[\"images\"][0]\n",
    "sample_path = VAL_IMG_DIR / sample_img_meta[\"file_name\"]\n",
    "\n",
    "preds = predict_top3(infer_model, sample_path, ckpt[\"internal_to_name\"], score_thr=CONFIG[\"score_threshold\"])\n",
    "\n",
    "print(\"Imagen:\", sample_path)\n",
    "if len(preds) == 0:\n",
    "    print(\"no se ha encontrado\")\n",
    "else:\n",
    "    for p in preds:\n",
    "        print(p)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
