{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8035d69a",
   "metadata": {},
   "source": [
    "# Tercer notebook\n",
    "En este notebook se uniran los archivos json creados con el dataset reducido para el entreno, se usara un modelo preentrenado Faster R-CNN por su precision y ahorro de recursos, ademas de empezar el registro de modelos en MLFLOW\n",
    "\n",
    "## Objetivos\n",
    "1. Cargar el dataset reducido generado en `data/processed/`.\n",
    "2. Crear un DataLoader compatible con modelos de detección (torchvision).\n",
    "3. Construir un modelo preentrenado (Faster R-CNN) y adaptar la cabeza a 3 clases.\n",
    "4. Entrenar en CPU con configuración eficiente.\n",
    "5. Registrar en MLflow:\n",
    "   - parámetros\n",
    "   - métricas\n",
    "   - artefactos (pesos, labelmap, config)\n",
    "6. Guardar un checkpoint local para usar en inferencia y en reentrenamiento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49098dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- Importa librerías necesarias para el entrenamiento.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import mlflow\n",
    "\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.transforms import functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d270675",
   "metadata": {},
   "source": [
    "1. Carga de archivos .json y validacion de estructura del proyecto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bed0371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROJECT_ROOT: C:\\Users\\Johnny\\Desktop\\IA-final\n",
      "TRAIN_JSON: C:\\Users\\Johnny\\Desktop\\IA-final\\data\\processed\\coco_person_car_airplane_train.json\n",
      "VAL_JSON  : C:\\Users\\Johnny\\Desktop\\IA-final\\data\\processed\\coco_person_car_airplane_val.json\n",
      "TARGET_CLASSES: ['person', 'car', 'airplane']\n",
      "target_cat_ids: [1, 3, 5]\n",
      "MODELS_DIR: C:\\Users\\Johnny\\Desktop\\IA-final\\models\\local_checkpoints\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "En esta secccion se  Carga project_config.json y labelmap.json creados en el Notebook 01.\n",
    "Aemas define rutas absolutas sin depender de la carpeta actual del notebook.\n",
    "\"\"\"\n",
    "\n",
    "def find_project_root(start: Path, max_up: int = 8) -> Path:\n",
    "    cur = start.resolve()\n",
    "    for _ in range(max_up):\n",
    "        if (cur / \"data\" / \"processed\" / \"project_config.json\").exists():\n",
    "            return cur\n",
    "        cur = cur.parent\n",
    "    raise FileNotFoundError(\"No se encontró data/processed/project_config.json. Ejecuta Notebook 01.\")\n",
    "\n",
    "PROJECT_ROOT = find_project_root(Path.cwd())\n",
    "PROCESSED_DIR = (PROJECT_ROOT / \"data\" / \"processed\").resolve()\n",
    "\n",
    "PROJECT_CONFIG_PATH = (PROCESSED_DIR / \"project_config.json\").resolve()\n",
    "LABELMAP_PATH = (PROCESSED_DIR / \"labelmap.json\").resolve()\n",
    "\n",
    "with open(PROJECT_CONFIG_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    project_config = json.load(f)\n",
    "\n",
    "with open(LABELMAP_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    labelmap = json.load(f)\n",
    "\n",
    "COCO_ROOT = Path(project_config[\"coco_root\"])\n",
    "TRAIN_IMG_DIR = Path(project_config[\"train_dir\"])\n",
    "VAL_IMG_DIR = Path(project_config[\"val_dir\"])\n",
    "\n",
    "TARGET_CLASSES = project_config[\"target_classes\"]\n",
    "target_cat_ids = labelmap[\"target_category_ids\"]\n",
    "\n",
    "TRAIN_JSON = (PROCESSED_DIR / \"coco_person_car_airplane_train.json\").resolve()\n",
    "VAL_JSON = (PROCESSED_DIR / \"coco_person_car_airplane_val.json\").resolve()\n",
    "\n",
    "MODELS_DIR = (PROJECT_ROOT / \"models\" / \"local_checkpoints\").resolve()\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Mismo contenido visto anteriormente, solo para confirmar las carpetas \")\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "print(\"TRAIN_JSON:\", TRAIN_JSON)\n",
    "print(\"VAL_JSON  :\", VAL_JSON)\n",
    "print(\"TARGET_CLASSES:\", TARGET_CLASSES)\n",
    "print(\"target_cat_ids:\", target_cat_ids)\n",
    "print(\"MODELS_DIR:\", MODELS_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa133bb",
   "metadata": {},
   "source": [
    "2. Configuracion de entrenamiento limitado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9016a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONFIG:\n",
      "seed: 42\n",
      "device: cpu\n",
      "cpu_num_threads: 11\n",
      "batch_size: 2\n",
      "num_workers: 0\n",
      "learning_rate: 0.0001\n",
      "weight_decay: 0.0001\n",
      "epochs: 2\n",
      "score_threshold: 0.5\n",
      "model_name: fasterrcnn_resnet50_fpn\n",
      "pretrained_weights: DEFAULT\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "En esta seccion se configurara el entrenaimeito en cpu, dado que no se cuenta con un gpu potente en la maquina donde se entrenara\n",
    "pero si cuentas con grafica de buena calidad puedes cambiar estos parametros para que tu entrenamiento se da mas rapido, ademas\n",
    "se establece threads y semillas para reproducibilidad, para evitar que toma imagenes aleatorias y se definen hiperparámetros conservadores para CPU.\n",
    "\"\"\"\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "CPU_NUM_THREADS = max(1, (os.cpu_count() or 2) - 1)\n",
    "torch.set_num_threads(CPU_NUM_THREADS)\n",
    "\n",
    "CONFIG = {\n",
    "    \"seed\": SEED,\n",
    "    \"device\": str(DEVICE),\n",
    "    \"cpu_num_threads\": CPU_NUM_THREADS,\n",
    "    \"batch_size\": 2,\n",
    "    \"num_workers\": 0,\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"weight_decay\": 1e-4,\n",
    "    \"epochs\": 2,\n",
    "    \"score_threshold\": 0.5,\n",
    "    \"model_name\": \"fasterrcnn_resnet50_fpn\",\n",
    "    \"pretrained_weights\": \"DEFAULT\",\n",
    "}\n",
    "\n",
    "print(\"CONFIG:\")\n",
    "for k, v in CONFIG.items():\n",
    "    print(f\"{k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4325b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN images: 3300\n",
      "VAL images  : 500\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Aqui principalemnte se verifica que TRAIN_JSON y VAL_JSON existan, ademas se usa el limite ya establecido de  cuántas imágenes contienen\n",
    "y en caso de ocurrir algo anormal, boquea ejecución si el train supera un límite \n",
    "\"\"\"\n",
    "\n",
    "def assert_file(path: Path, label: str) -> None:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Archivo requerido no encontrado: {label}: {path}\")\n",
    "\n",
    "assert_file(TRAIN_JSON, \"TRAIN_JSON reducido (Notebook 02)\")\n",
    "assert_file(VAL_JSON, \"VAL_JSON reducido (Notebook 02)\")\n",
    "\n",
    "with open(TRAIN_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    train_coco = json.load(f)\n",
    "\n",
    "with open(VAL_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    val_coco = json.load(f)\n",
    "\n",
    "train_images_n = len(train_coco[\"images\"])\n",
    "val_images_n = len(val_coco[\"images\"])\n",
    "\n",
    "print(\"TRAIN images:\", train_images_n)\n",
    "print(\"VAL images  :\", val_images_n)\n",
    "\n",
    "MAX_ALLOWED_TRAIN_IMAGES = 10000  # seguridad dura\n",
    "if train_images_n > MAX_ALLOWED_TRAIN_IMAGES:\n",
    "    raise RuntimeError(\n",
    "        f\"TRAIN_JSON tiene {train_images_n} imágenes, supera el límite {MAX_ALLOWED_TRAIN_IMAGES}. \"\n",
    "        \"Regenera el JSON en Notebook 02 con recorte.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f5550b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "En esta sección se define un Dataset personalizado el que ya fue reducido en el segundo notebook, para trabajar con datos en formato COCO,\n",
    "adaptado específicamente para modelos de detección de objetos de torchvision \n",
    "El objetivo principal es cargar las imágenes, procesar sus anotaciones y devolver los datos en el formato exacto que el modelo necesita para entrenar.\n",
    "\"\"\"\n",
    "\n",
    "class CocoReducedDetectionDataset(Dataset):\n",
    "    def __init__(self, images_dir: Path, coco_json: dict):\n",
    "        self.images_dir = images_dir\n",
    "        self.coco = coco_json\n",
    "\n",
    "        self.images = self.coco[\"images\"]\n",
    "        self.annotations = self.coco[\"annotations\"]\n",
    "\n",
    "        self.img_id_to_anns = {}\n",
    "        for ann in self.annotations:\n",
    "            self.img_id_to_anns.setdefault(ann[\"image_id\"], []).append(ann)\n",
    "\n",
    "        self.id_to_image = {img[\"id\"]: img for img in self.images}\n",
    "        self.image_ids = list(self.id_to_image.keys())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        img_id = self.image_ids[idx]\n",
    "        img_meta = self.id_to_image[img_id]\n",
    "        img_path = self.images_dir / img_meta[\"file_name\"]\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        img_tensor = F.to_tensor(img)\n",
    "\n",
    "        anns = self.img_id_to_anns.get(img_id, [])\n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        areas = []\n",
    "        iscrowd = []\n",
    "\n",
    "        for a in anns:\n",
    "            x, y, w, h = a[\"bbox\"]\n",
    "            boxes.append([x, y, x + w, y + h])\n",
    "            labels.append(a[\"category_id\"])\n",
    "            areas.append(a.get(\"area\", w * h))\n",
    "            iscrowd.append(a.get(\"iscrowd\", 0))\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": torch.tensor(boxes, dtype=torch.float32),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.int64),\n",
    "            \"image_id\": torch.tensor([img_id], dtype=torch.int64),\n",
    "            \"area\": torch.tensor(areas, dtype=torch.float32),\n",
    "            \"iscrowd\": torch.tensor(iscrowd, dtype=torch.int64),\n",
    "        }\n",
    "\n",
    "        return img_tensor, target\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images, targets = zip(*batch)\n",
    "    return list(images), list(targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85975dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 3300\n",
      "Val samples  : 500\n",
      "Train iters per epoch: 1650\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "En esta sección se crean los datasets de entrenamiento y validación, y luego se construyen los DataLoaders que serán usados durante el entrenamiento del modelo.\n",
    "tabien garantiza que train_loader y val_loader existan antes de entrenar.\n",
    "\"\"\"\n",
    "\n",
    "train_ds = CocoReducedDetectionDataset(TRAIN_IMG_DIR, train_coco)\n",
    "val_ds = CocoReducedDetectionDataset(VAL_IMG_DIR, val_coco)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=CONFIG[\"batch_size\"],\n",
    "    shuffle=True,\n",
    "    num_workers=CONFIG[\"num_workers\"],\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=CONFIG[\"num_workers\"],\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "print(\"Train samples:\", len(train_ds))\n",
    "print(\"Val samples  :\", len(val_ds))\n",
    "print(\"Train iters per epoch:\", len(train_loader))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cc2f4c",
   "metadata": {},
   "source": [
    "2. Consuntruccion del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269ca459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM_CLASSES: 4\n",
      "coco_to_internal: {1: 1, 3: 2, 5: 3}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "En esta sección se construye el modelo de detección Faster R-CNN utilizando pesos preentrenados\n",
    " y se ajusta su arquitectura para trabajar únicamente con las clases de interés del proyecto.\n",
    "Debido a que los identificadores de categoría en COCO no son consecutivos ni empiezan desde 1,\n",
    "se realiza un remapeo de los category_id originales a etiquetas internas comprendidas entre 1..K,\n",
    "dejando el valor 0 reservado para el background, tal como lo requiere el modelo.\n",
    "Asimismo, se redefine la cabeza de clasificación del modelo para que el número de salidas coincida con el total de clases objetivo más la clase de fondo.\n",
    "\"\"\"\n",
    "\n",
    "target_cat_ids_int = [int(x) for x in target_cat_ids]\n",
    "coco_to_internal = {cid: i + 1 for i, cid in enumerate(target_cat_ids_int)}\n",
    "internal_to_name = {i + 1: name for i, name in enumerate(TARGET_CLASSES)}\n",
    "\n",
    "NUM_CLASSES = len(TARGET_CLASSES) + 1\n",
    "\n",
    "def remap_targets(targets: List[Dict]) -> List[Dict]:\n",
    "    out = []\n",
    "    for t in targets:\n",
    "        new_t = t.copy()\n",
    "        labels = t[\"labels\"].clone()\n",
    "        for i in range(labels.shape[0]):\n",
    "            labels[i] = coco_to_internal[int(labels[i].item())]\n",
    "        new_t[\"labels\"] = labels\n",
    "        out.append(new_t)\n",
    "    return out\n",
    "\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=CONFIG[\"pretrained_weights\"])\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, NUM_CLASSES)\n",
    "\n",
    "model.to(DEVICE)\n",
    "model.train()\n",
    "\n",
    "print(\"NUM_CLASSES:\", NUM_CLASSES)\n",
    "print(\"coco_to_internal:\", coco_to_internal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b774294",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "En esta sección se define el optimizador AdamW, el cual se utiliza para actualizar los pesos del modelo durante el entrenamiento, ya que ofrece buena estabilidad y control del sobreajuste mediante weight_decay.\n",
    "Además, se implementan las funciones de entrenamiento por época (train_one_epoch) y evaluación (evaluate_loss), donde se calcula la pérdida promedio por época, lo cual es especialmente útil para el seguimiento del\n",
    "entrenamiento en CPU y para comparar el desempeño entre entrenamiento y validación.\n",
    "\"\"\"\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=CONFIG[\"learning_rate\"],\n",
    "    weight_decay=CONFIG[\"weight_decay\"]\n",
    ")\n",
    "\n",
    "def train_one_epoch(model, data_loader, optimizer, epoch: int) -> float:\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    n = 0\n",
    "\n",
    "    for images, targets in tqdm(data_loader, desc=f\"train e{epoch}\", leave=False):\n",
    "        images = [img.to(DEVICE) for img in images]\n",
    "        targets = remap_targets(targets)\n",
    "        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += float(losses.item())\n",
    "        n += 1\n",
    "\n",
    "    return total_loss / max(1, n)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_loss(model, data_loader) -> float:\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    n = 0\n",
    "\n",
    "    for images, targets in tqdm(data_loader, desc=\"val\", leave=False):\n",
    "        images = [img.to(DEVICE) for img in images]\n",
    "        targets = remap_targets(targets)\n",
    "        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        total_loss += float(losses.item())\n",
    "        n += 1\n",
    "\n",
    "    model.train()\n",
    "    return total_loss / max(1, n)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4217cc31",
   "metadata": {},
   "source": [
    "3. Uso del entrenamiento con dataset reducido, modelo preentrenado y optimizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "007961d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/02/01 08:34:46 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.schemas\n",
      "2026/02/01 08:34:46 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.tables\n",
      "2026/02/01 08:34:46 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.types\n",
      "2026/02/01 08:34:46 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.constraints\n",
      "2026/02/01 08:34:46 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.defaults\n",
      "2026/02/01 08:34:46 INFO alembic.runtime.plugins: setup plugin alembic.autogenerate.comments\n",
      "2026/02/01 08:34:48 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n",
      "2026/02/01 08:34:48 INFO mlflow.store.db.utils: Updating database tables\n",
      "2026/02/01 08:34:48 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n",
      "2026/02/01 08:34:48 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n",
      "2026/02/01 08:34:48 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n",
      "2026/02/01 08:34:48 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n",
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2 | train_loss=0.4747 | val_loss=0.4836 | time=21692.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2 | train_loss=0.4070 | val_loss=0.4406 | time=20395.6s\n",
      "Entrenamiento finalizado\n",
      "best_val_loss: 0.44062818501889706\n",
      "best_ckpt_path: C:\\Users\\Johnny\\Desktop\\IA-final\\models\\local_checkpoints\\best_frcnn_cpu_base_train_20260201_083448.pt\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Esta celda:\n",
    "- Configura MLflow con SQLite (mlflow.db) para evitar deprecación de FileStore.\n",
    "- Registra params, métricas y artefactos (mappings + config).\n",
    "- Entrena en CPU y GUARDA CHECKPOINT POR ÉPOCA (para no perder progreso si falla algo).\n",
    "- Calcula val_loss correctamente (en torchvision detection el loss sale en modo train, sin grad).\n",
    "- Guarda además el mejor modelo (best) por menor val_loss.\n",
    "\"\"\"\n",
    "\n",
    "# -----------------------------\n",
    "# MLflow: SQLite backend\n",
    "# -----------------------------\n",
    "MLFLOW_DB = (PROJECT_ROOT / \"mlflow.db\").resolve()\n",
    "mlflow.set_tracking_uri(f\"sqlite:///{MLFLOW_DB.as_posix()}\")\n",
    "\n",
    "EXPERIMENT_NAME = \"object_detection_coco_cpu\"\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "run_name = f\"base_train_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "best_ckpt_path = None\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_loss(model, data_loader) -> float:\n",
    "    \"\"\"\n",
    "    Esta función:\n",
    "    - Calcula pérdida en validación sin gradientes.\n",
    "    - En modelos de detección torchvision, el diccionario de pérdidas se obtiene en modo train.\n",
    "    - Por eso forzamos model.train() temporalmente y luego restauramos el estado previo.\n",
    "    \"\"\"\n",
    "    was_training = model.training\n",
    "    model.train()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    n = 0\n",
    "\n",
    "    for images, targets in tqdm(data_loader, desc=\"val\", leave=False):\n",
    "        images = [img.to(DEVICE) for img in images]\n",
    "        targets = remap_targets(targets)\n",
    "        targets = [{k: v.to(DEVICE) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)  # dict de pérdidas\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        total_loss += float(losses.item())\n",
    "        n += 1\n",
    "\n",
    "    if not was_training:\n",
    "        model.eval()\n",
    "\n",
    "    return total_loss / max(1, n)\n",
    "\n",
    "\n",
    "with mlflow.start_run(run_name=run_name):\n",
    "    # -----------------------------\n",
    "    # Params + tags\n",
    "    # -----------------------------\n",
    "    mlflow.log_params(CONFIG)\n",
    "    mlflow.set_tag(\"stage\", \"base\")\n",
    "    mlflow.set_tag(\"train_type\", \"cpu\")\n",
    "    mlflow.set_tag(\"classes\", \",\".join(TARGET_CLASSES))\n",
    "    mlflow.set_tag(\"model_arch\", CONFIG[\"model_name\"])\n",
    "\n",
    "    # -----------------------------\n",
    "    # Artefactos de configuración/mapping\n",
    "    # -----------------------------\n",
    "    mapping_artifact = {\n",
    "        \"target_classes\": TARGET_CLASSES,\n",
    "        \"target_category_ids\": target_cat_ids_int,\n",
    "        \"coco_to_internal\": coco_to_internal,\n",
    "        \"internal_to_name\": internal_to_name,\n",
    "        \"train_json\": str(TRAIN_JSON.as_posix()),\n",
    "        \"val_json\": str(VAL_JSON.as_posix()),\n",
    "    }\n",
    "\n",
    "    mapping_path = MODELS_DIR / f\"mapping_{run_name}.json\"\n",
    "    with open(mapping_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(mapping_artifact, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    mlflow.log_artifact(str(mapping_path), artifact_path=\"artifacts\")\n",
    "    mlflow.log_artifact(str(PROJECT_CONFIG_PATH), artifact_path=\"artifacts\")\n",
    "    mlflow.log_artifact(str(LABELMAP_PATH), artifact_path=\"artifacts\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # Entrenamiento por épocas\n",
    "    # -----------------------------\n",
    "    for epoch in range(1, CONFIG[\"epochs\"] + 1):\n",
    "        t0 = time.time()\n",
    "\n",
    "        # TRAIN\n",
    "        train_loss = train_one_epoch(model, train_loader, optimizer, epoch)\n",
    "\n",
    "        # CHECKPOINT por época (SIEMPRE, antes de validar)\n",
    "        epoch_ckpt_path = MODELS_DIR / f\"epoch_{epoch}_frcnn_cpu_{run_name}.pt\"\n",
    "        epoch_ckpt = {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"config\": CONFIG,\n",
    "            \"target_classes\": TARGET_CLASSES,\n",
    "            \"target_category_ids\": target_cat_ids_int,\n",
    "            \"coco_to_internal\": coco_to_internal,\n",
    "            \"internal_to_name\": internal_to_name,\n",
    "        }\n",
    "        torch.save(epoch_ckpt, epoch_ckpt_path)\n",
    "        mlflow.log_artifact(str(epoch_ckpt_path), artifact_path=\"checkpoints\")\n",
    "\n",
    "        # VALIDATION (loss)\n",
    "        val_loss = evaluate_loss(model, val_loader)\n",
    "\n",
    "        epoch_time = time.time() - t0\n",
    "\n",
    "        # LOG metrics\n",
    "        mlflow.log_metric(\"train_loss\", train_loss, step=epoch)\n",
    "        mlflow.log_metric(\"val_loss\", val_loss, step=epoch)\n",
    "        mlflow.log_metric(\"epoch_time_sec\", epoch_time, step=epoch)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch}/{CONFIG['epochs']} | \"\n",
    "            f\"train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | time={epoch_time:.1f}s\"\n",
    "        )\n",
    "\n",
    "        # BEST model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_ckpt_path = MODELS_DIR / f\"best_frcnn_cpu_{run_name}.pt\"\n",
    "\n",
    "            best_ckpt = epoch_ckpt.copy()\n",
    "            best_ckpt[\"best_val_loss\"] = best_val_loss\n",
    "\n",
    "            torch.save(best_ckpt, best_ckpt_path)\n",
    "            mlflow.log_artifact(str(best_ckpt_path), artifact_path=\"checkpoints\")\n",
    "\n",
    "    mlflow.log_metric(\"best_val_loss\", best_val_loss)\n",
    "\n",
    "print(\"Entrenamiento finalizado\")\n",
    "print(\"best_val_loss:\", best_val_loss)\n",
    "print(\"best_ckpt_path:\", best_ckpt_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf35bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to C:\\Users\\Johnny/.cache\\torch\\hub\\checkpoints\\resnet50-0676ba61.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 97.8M/97.8M [00:03<00:00, 31.1MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagen: C:\\Users\\Johnny\\Desktop\\IA-final\\data\\archive\\coco2017\\val2017\\000000397133.jpg\n",
      "{'name': 'person', 'score': 0.9935327768325806, 'box_xyxy': [385.1822204589844, 76.41061401367188, 497.1370544433594, 343.8106994628906]}\n",
      "{'name': 'person', 'score': 0.9191341400146484, 'box_xyxy': [352.98193359375, 97.65465545654297, 404.5033264160156, 238.57272338867188]}\n",
      "{'name': 'person', 'score': 0.6582873463630676, 'box_xyxy': [199.18922424316406, 9.169189453125, 283.14971923828125, 101.2300796508789]}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Esta seccion se usa mas por seguridad del modelo para la interfaz,carga el mejor checkpoint\n",
    "despues ejecuta inferencia sobre una imagen del set val y por ultimo devuelve hasta 3 detecciones con mayor score.\n",
    "\"\"\"\n",
    "\n",
    "@torch.no_grad()\n",
    "def load_model_from_ckpt(ckpt_path: Path):\n",
    "    ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=None)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    num_classes = len(ckpt[\"target_classes\"]) + 1\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "    model.eval()\n",
    "    return model, ckpt\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_top3(model, image_path: Path, internal_to_name: Dict[int, str], score_thr: float = 0.5):\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    img_t = F.to_tensor(img)\n",
    "\n",
    "    outputs = model([img_t])[0]\n",
    "    boxes = outputs[\"boxes\"].cpu()\n",
    "    scores = outputs[\"scores\"].cpu()\n",
    "    labels = outputs[\"labels\"].cpu()\n",
    "\n",
    "    keep = scores >= score_thr\n",
    "    boxes = boxes[keep]\n",
    "    scores = scores[keep]\n",
    "    labels = labels[keep]\n",
    "\n",
    "    if len(scores) == 0:\n",
    "        return []\n",
    "\n",
    "    topk = min(3, len(scores))\n",
    "    idx = torch.argsort(scores, descending=True)[:topk]\n",
    "\n",
    "    results = []\n",
    "    for i in idx:\n",
    "        lbl = int(labels[i].item())\n",
    "        results.append({\n",
    "            \"name\": internal_to_name.get(lbl, \"unknown\"),\n",
    "            \"score\": float(scores[i].item()),\n",
    "            \"box_xyxy\": boxes[i].tolist(),\n",
    "        })\n",
    "    return results\n",
    "\n",
    "if best_ckpt_path is None:\n",
    "    raise FileNotFoundError(\"No hay best_ckpt_path. Revisa el entrenamiento.\")\n",
    "\n",
    "infer_model, ckpt = load_model_from_ckpt(best_ckpt_path)\n",
    "\n",
    "sample_img_meta = val_coco[\"images\"][0]\n",
    "sample_path = VAL_IMG_DIR / sample_img_meta[\"file_name\"]\n",
    "\n",
    "preds = predict_top3(infer_model, sample_path, ckpt[\"internal_to_name\"], score_thr=CONFIG[\"score_threshold\"])\n",
    "\n",
    "print(\"Imagen:\", sample_path)\n",
    "if len(preds) == 0:\n",
    "    print(\"no se ha encontrado\")\n",
    "else:\n",
    "    for p in preds:\n",
    "        print(p)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
