{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a33e5f6",
   "metadata": {},
   "source": [
    "# First Notebook\n",
    "\n",
    "In this first notebook, key concepts for the rest of the project are defined, including the selected dataset, the main directory paths used for communication between notebooks, and the preprocessing structure.\n",
    "\n",
    "The dataset used in this project requires generating reduced `.json` annotation files for efficient processing and CPU-based training.\n",
    "\n",
    "The base dataset selected is **COCO 2017**, which can be publicly downloaded from the following link:\n",
    "\n",
    "https://www.kaggle.com/datasets/awsaf49/coco-2017-dataset\n",
    "\n",
    "For this project, the dataset is assumed to be already downloaded locally; therefore, it will not be automatically downloaded within this repository.\n",
    "\n",
    "To reproduce the results, you must manually download the dataset from the link above and place it in the following directory:\n",
    "\n",
    "IA-final/data/archive/coco2017\n",
    "\n",
    "\n",
    "## Objectives\n",
    "\n",
    "1. Verify the structure of the COCO2017 dataset located at:\n",
    "\n",
    "   data/archive/coco2017\n",
    "\n",
    "2. Confirm that all required files and folders exist and are properly organized.\n",
    "\n",
    "3. Load the annotation files:\n",
    "   - instances_train2017.json\n",
    "   - instances_val2017.json\n",
    "\n",
    "4. Define three target classes to enable efficient CPU-based training.\n",
    "\n",
    "5. Generate and save a `labelmap.json` file containing the selected target classes for use in subsequent notebooks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b2f56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- Import standard libraries used throughout the notebook.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef012d46",
   "metadata": {},
   "source": [
    "## 1. Creation and Validation of Essential Project Directories\n",
    "\n",
    "This section defines and validates the essential project directories required for the workflow. It ensures that all necessary folders exist and are properly structured before proceeding with dataset processing and model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4c6aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Para que confirmes que los datos mas improtantes se encuentren tras tu descarga se debe confirmar que todos estos archvios existan\n",
      "TRAIN_DIR: C:\\Users\\Johnny\\Desktop\\IA-final\\data\\archive\\coco2017\\train2017\n",
      "VAL_DIR  : C:\\Users\\Johnny\\Desktop\\IA-final\\data\\archive\\coco2017\\val2017\n",
      "ANN_DIR  : C:\\Users\\Johnny\\Desktop\\IA-final\\data\\archive\\coco2017\\annotations\n",
      "TRAIN_ANN: C:\\Users\\Johnny\\Desktop\\IA-final\\data\\archive\\coco2017\\annotations\\instances_train2017.json\n",
      "VAL_ANN  : C:\\Users\\Johnny\\Desktop\\IA-final\\data\\archive\\coco2017\\annotations\\instances_val2017.json\n",
      "PROCESSED_DIR: C:\\Users\\Johnny\\Desktop\\IA-final\\data\\processed\n",
      "Clases seleccionadas: ['person', 'car', 'airplane']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "In this first section:\n",
    "- Defines the COCO2017 dataset paths in a unique and centralized way.\n",
    "- Sets fixed target classes: person, car, airplane. These classes were selected because they have a large number of instances in the dataset.\n",
    "- Creates the data/processed directory if it does not exist.\n",
    "- Stores the absolute path to avoid conflicts in notebooks.\n",
    "\"\"\"\n",
    "PROJECT_ROOT = Path(\"..\")     \n",
    "\n",
    "COCO_ROOT = PROJECT_ROOT / \"data\" / \"archive\" / \"coco2017\"\n",
    "\n",
    "TRAIN_DIR = (COCO_ROOT / \"train2017\").resolve()\n",
    "VAL_DIR = (COCO_ROOT / \"val2017\").resolve()\n",
    "ANN_DIR = (COCO_ROOT / \"annotations\").resolve()\n",
    "\n",
    "TRAIN_ANN = (ANN_DIR / \"instances_train2017.json\").resolve()\n",
    "VAL_ANN = (ANN_DIR / \"instances_val2017.json\").resolve()\n",
    "\n",
    "PROCESSED_DIR = (PROJECT_ROOT / \"data\" / \"processed\").resolve()\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "PROJECT_CONFIG_PATH = (PROCESSED_DIR / \"project_config.json\").resolve()\n",
    "LABELMAP_PATH = (PROCESSED_DIR / \"labelmap.json\").resolve()\n",
    "\n",
    "TARGET_CLASSES = [\"person\", \"car\", \"airplane\"]\n",
    "\n",
    "print(\"Para que confirmes que los datos mas improtantes se encuentren tras tu descarga se debe confirmar que todos estos archvios existan\")\n",
    "print(\"TRAIN_DIR:\", TRAIN_DIR)\n",
    "print(\"VAL_DIR  :\", VAL_DIR)\n",
    "print(\"ANN_DIR  :\", ANN_DIR)\n",
    "print(\"TRAIN_ANN:\", TRAIN_ANN)\n",
    "print(\"VAL_ANN  :\", VAL_ANN)\n",
    "print(\"PROCESSED_DIR:\", PROCESSED_DIR)\n",
    "print(\"Clases seleccionadas:\", TARGET_CLASSES)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32b4421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK: COCO_ROOT: ..\\data\\archive\\coco2017\n",
      "OK: train2017: C:\\Users\\Johnny\\Desktop\\IA-final\\data\\archive\\coco2017\\train2017\n",
      "OK: val2017: C:\\Users\\Johnny\\Desktop\\IA-final\\data\\archive\\coco2017\\val2017\n",
      "OK: annotations: C:\\Users\\Johnny\\Desktop\\IA-final\\data\\archive\\coco2017\\annotations\n",
      "OK: instances_train2017.json: C:\\Users\\Johnny\\Desktop\\IA-final\\data\\archive\\coco2017\\annotations\\instances_train2017.json\n",
      "OK: instances_val2017.json: C:\\Users\\Johnny\\Desktop\\IA-final\\data\\archive\\coco2017\\annotations\\instances_val2017.json\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This cell:\n",
    "Performs a simple validation of the previously defined directories to prevent future path conflicts.  \n",
    "To avoid issues, all paths are declared from the first notebook.\n",
    "\"\"\"\n",
    "\n",
    "def assert_exists(path: Path, label: str) -> None:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Recurso requerido no encontrado: {label}: {path}\")\n",
    "    print(f\"OK: {label}: {path}\")\n",
    "\n",
    "assert_exists(COCO_ROOT, \"COCO_ROOT\")\n",
    "assert_exists(TRAIN_DIR, \"train2017\")\n",
    "assert_exists(VAL_DIR, \"val2017\")\n",
    "assert_exists(ANN_DIR, \"annotations\")\n",
    "assert_exists(TRAIN_ANN, \"instances_train2017.json\")\n",
    "assert_exists(VAL_ANN, \"instances_val2017.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025d2ccb",
   "metadata": {},
   "source": [
    "## 2. Dataset description and JSON creation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e289622f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train JSON cargado\n",
      "Categorias: 80\n",
      "Imagenes  : 118287\n",
      "Anotaciones: 860001\n",
      "TARGET_CLASSES: ['person', 'car', 'airplane']\n",
      "target_cat_ids: [1, 3, 5]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Dataset description and JSON creation:\n",
    "- Loads the training annotations JSON file, which is required for dataset optimization.\n",
    "- Extracts categories and creates id <-> name mappings.\n",
    "- Verifies that the target classes exist in the COCO dataset.\n",
    "\"\"\"\n",
    "\n",
    "with open(TRAIN_ANN, \"r\", encoding=\"utf-8\") as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "categories = train_data[\"categories\"]\n",
    "images = train_data[\"images\"]\n",
    "annotations = train_data[\"annotations\"]\n",
    "\n",
    "cat_id_to_name = {c[\"id\"]: c[\"name\"] for c in categories}\n",
    "cat_name_to_id = {c[\"name\"]: c[\"id\"] for c in categories}\n",
    "\n",
    "missing = [c for c in TARGET_CLASSES if c not in cat_name_to_id]\n",
    "if missing:\n",
    "    raise ValueError(\n",
    "        \"Estas clases objetivo no existen en COCO categories: \" + \", \".join(missing)\n",
    "    )\n",
    "\n",
    "target_cat_ids = [cat_name_to_id[c] for c in TARGET_CLASSES]\n",
    "\n",
    "print(\"Train JSON cargado\")\n",
    "print(\"Categorias:\", len(categories))\n",
    "print(\"Imagenes  :\", len(images))\n",
    "print(\"Anotaciones:\", len(annotations))\n",
    "print(\"TARGET_CLASSES:\", TARGET_CLASSES)\n",
    "print(\"target_cat_ids:\", target_cat_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576015d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anotaciones por clase objetivo (train):\n",
      "person: 262465\n",
      "car: 43867\n",
      "airplane: 5135\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "- Counts how many annotations exist in the training set for each target class.\n",
    "- Serves as a sanity check to ensure there is sufficient data. It is important to note that, due to resource constraints, only a fraction of the dataset was used for training.\n",
    "\"\"\"\n",
    "\n",
    "counter = Counter()\n",
    "for ann in annotations:\n",
    "    cid = ann[\"category_id\"]\n",
    "    if cid in set(target_cat_ids):\n",
    "        counter[cid] += 1\n",
    "\n",
    "print(\"Anotaciones por clase objetivo (train):\")\n",
    "for cls_name, cid in zip(TARGET_CLASSES, target_cat_ids):\n",
    "    print(f\"{cls_name}: {counter[cid]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76a16a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado: C:\\Users\\Johnny\\Desktop\\IA-final\\data\\processed\\project_config.json\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Here we start creating reusable code for the rest of the notebooks.\n",
    "- Saves a single project configuration with absolute paths.\n",
    "- The idea is that all notebooks read this file and do not redefine paths manually.\n",
    "\"\"\"\n",
    "\n",
    "project_config = {\n",
    "    \"project_root\": str(PROJECT_ROOT.as_posix()),\n",
    "    \"coco_root\": str(COCO_ROOT.as_posix()),\n",
    "    \"train_dir\": str(TRAIN_DIR.as_posix()),\n",
    "    \"val_dir\": str(VAL_DIR.as_posix()),\n",
    "    \"ann_dir\": str(ANN_DIR.as_posix()),\n",
    "    \"train_ann\": str(TRAIN_ANN.as_posix()),\n",
    "    \"val_ann\": str(VAL_ANN.as_posix()),\n",
    "    \"processed_dir\": str(PROCESSED_DIR.as_posix()),\n",
    "    \"target_classes\": TARGET_CLASSES,\n",
    "}\n",
    "\n",
    "with open(PROJECT_CONFIG_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(project_config, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"Guardado:\", PROJECT_CONFIG_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ae202b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guardado: C:\\Users\\Johnny\\Desktop\\IA-final\\data\\processed\\labelmap.json\n",
      "{'dataset': 'COCO2017', 'target_classes': ['person', 'car', 'airplane'], 'target_category_ids': [1, 3, 5], 'name_to_id': {'person': 1, 'car': 3, 'airplane': 5}, 'id_to_name': {'1': 'person', '3': 'car', '5': 'airplane'}}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "In this section, the following steps are performed:\n",
    "- Saves a minimal and stable label map for the project.\n",
    "- Includes:\n",
    "  - dataset\n",
    "  - target_classes\n",
    "  - name_to_id and id_to_name only for the target classes\n",
    "  - target_category_ids\n",
    "\"\"\"\n",
    "\n",
    "labelmap = {\n",
    "    \"dataset\": \"COCO2017\",\n",
    "    \"target_classes\": TARGET_CLASSES,\n",
    "    \"target_category_ids\": target_cat_ids,\n",
    "    \"name_to_id\": {name: cat_name_to_id[name] for name in TARGET_CLASSES},\n",
    "    \"id_to_name\": {str(cat_name_to_id[name]): name for name in TARGET_CLASSES},\n",
    "}\n",
    "\n",
    "with open(LABELMAP_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(labelmap, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"Guardado:\", LABELMAP_PATH)\n",
    "print(labelmap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d43731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "project_config.json OK: ../data/archive/coco2017\n",
      "labelmap.json OK: ['person', 'car', 'airplane']\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This cell:\n",
    "- Reloads project_config.json and labelmap.json to ensure they were saved correctly.\n",
    "- This prevents situations where the notebook appears to work, but nothing is actually saved.\n",
    "\"\"\"\n",
    "\n",
    "with open(PROJECT_CONFIG_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    cfg_check = json.load(f)\n",
    "\n",
    "with open(LABELMAP_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    lm_check = json.load(f)\n",
    "\n",
    "print(\"project_config.json OK:\", cfg_check[\"coco_root\"])\n",
    "print(\"labelmap.json OK:\", lm_check[\"target_classes\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
